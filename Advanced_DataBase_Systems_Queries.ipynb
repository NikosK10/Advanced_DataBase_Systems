{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91a0962-47ae-44ab-8e1b-adf7e749a41f",
   "metadata": {},
   "source": [
    "\n",
    "# Query 1 – Victim age groups in aggravated assault incidents\n",
    "\n",
    "Goal: For all crime incidents that involve any form of “aggravated assault”, compute the number of victims per age group and display the age groups in descending order of incident count.\n",
    "Age groups:\n",
    "\n",
    "- Children: < 18\n",
    "\n",
    "- Young Adults: 18–24\n",
    "\n",
    "- Adults: 25–64\n",
    "\n",
    "- Elderly: > 64\n",
    "\n",
    "Below are three implementations of the same query:\n",
    "\n",
    "- DataFrame API with a Python UDF\n",
    "\n",
    "- DataFrame API with native Spark expressions (no UDF)\n",
    "\n",
    "- RDD API implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323a93b-28be-46bb-bf9d-f88558007b86",
   "metadata": {},
   "source": [
    "## Query 1 – DataFrame API with UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d596d2d8-c83c-4e87-be09-7bc0c2c5daf1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>107</td><td>application_1761923966900_0122</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0122/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-146.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0122_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8bd5319d564712b71de7f1bbfdd693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef11439c32864377a1d3000b1b39420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Loaded!\n",
      "\n",
      "+------------+------+\n",
      "|   Age Group| count|\n",
      "+------------+------+\n",
      "|      Adults|121660|\n",
      "|Young Adults| 33758|\n",
      "|    Children| 10904|\n",
      "|     Elderly|  6011|\n",
      "|     Unknown|  5110|\n",
      "+------------+------+\n",
      "\n",
      "Execution time: 11.92 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "import time\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query1\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema definition for the LA crime dataset\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# UDF for mapping victim age to an age group\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def age_group(age):\n",
    "    try:\n",
    "        age = int(age)\n",
    "        if age<=0:\n",
    "            return \"Unknown\"\n",
    "        elif 0< age < 18:\n",
    "            return \"Children\"\n",
    "        elif 18 <= age <= 24:\n",
    "            return \"Young Adults\"\n",
    "        elif 25 <= age <= 64:\n",
    "            return \"Adults\"\n",
    "        elif age > 64:\n",
    "            return \"Elderly\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "age_group_udf = udf(age_group, StringType())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Execution timing starts\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load crime CSV files from S3 and union them into a single DataFrame\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "print(\"CSV Loaded!\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Filter only incidents whose description contains \"AGGRAVATED ASSAULT\"\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "aggr_assault_df = crime_df.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Add \"Age Group\" column using the Python UDF\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "aggr_assault_df = aggr_assault_df.withColumn(\"Age Group\", age_group_udf(col(\"Vict Age\")))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Group by age group and sort by descending incident count\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_df = aggr_assault_df.groupBy(\"Age Group\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Display results and execution time\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d3fd2-03aa-44c1-b77a-f43c083ec4dc",
   "metadata": {},
   "source": [
    "## Query 1 – DataFrame API without UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8bb9f9-4993-4a7e-8150-87ed25637dbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>108</td><td>application_1761923966900_0123</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0123/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-97.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0123_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c328228dfc1d4a1abb6d525ba6f6c023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1bdbe933ef43d694efc66a6ef73f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Loaded!\n",
      "\n",
      "+------------+------+\n",
      "|   Age Group| count|\n",
      "+------------+------+\n",
      "|      Adults|121660|\n",
      "|Young Adults| 33758|\n",
      "|    Children| 10904|\n",
      "|     Elderly|  6011|\n",
      "|     Unknown|  5110|\n",
      "+------------+------+\n",
      "\n",
      "Execution time: 10.56 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import col, when\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query1\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema definition for the LA crime dataset\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Execution timing starts\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load crime CSV files from S3 and union them into a single DataFrame\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "print(\"CSV Loaded!\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Filter only incidents whose description contains \"AGGRAVATED ASSAULT\"\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "aggr_assault_df = crime_df.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Add \"Age Group\" column using only native Spark expressions\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "aggr_assault_df = aggr_assault_df.withColumn(\n",
    "    \"Age Group\",\n",
    "    when(col(\"Vict Age\") <=0, \"Unknown\")\n",
    "    .when( (0 < col(\"Vict Age\")) & (col(\"Vict Age\") < 18), \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Elderly\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Group by age group and sort by descending incident count\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_df = aggr_assault_df.groupBy(\"Age Group\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Display results and execution time\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c2737-049e-44a2-a05a-ea9d4ed0de15",
   "metadata": {},
   "source": [
    "## Query 1 – RDD API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3803c740-3ee8-494e-9a02-b858dce6ae47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>110</td><td>application_1761923966900_0125</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0125/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-43.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0125_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7e88c427c343f6ab246eebfe98aa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf0fea1f97a497ca3da99be2a4fbd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Adults', 121660)\n",
      "('Young Adults', 33758)\n",
      "('Children', 10904)\n",
      "('Elderly', 6011)\n",
      "('Unknown', 5110)\n",
      "\n",
      "Execution time (RDD CSV-safe): 16.45 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query1 RDD CSV-safe\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper function: map age to age group\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def age_group_from_age(age):\n",
    "    try:\n",
    "        age = int(age)\n",
    "        if age<=0:\n",
    "            return \"Unknown\"\n",
    "        elif 0< age < 18:\n",
    "            return \"Children\"\n",
    "        elif 18 <= age <= 24:\n",
    "            return \"Young Adults\"\n",
    "        elif 25 <= age <= 64:\n",
    "            return \"Adults\"\n",
    "        elif age > 64:\n",
    "            return \"Elderly\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper function: CSV-safe splitting of a line\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def safe_split(line):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Execution timing starts\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Read both CSV files as RDDs of raw lines\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_rdd1 = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    ")\n",
    "crime_rdd2 = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Remove headers from both RDDs\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "header1 = crime_rdd1.first()\n",
    "header2 = crime_rdd2.first()\n",
    "crime_rdd1 = crime_rdd1.filter(lambda x: x != header1)\n",
    "crime_rdd2 = crime_rdd2.filter(lambda x: x != header2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Union the two RDDs and parse CSV lines safely\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_rdd = crime_rdd1.union(crime_rdd2)\n",
    "\n",
    "crime_rdd = crime_rdd.map(safe_split)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Filter for \"AGGRAVATED ASSAULT\" incidents\n",
    "# Crm Cd Desc is at index 9, Vict Age at index 11\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "aggr_assault_rdd = crime_rdd.filter(lambda x: len(x) > 11 and \"AGGRAVATED ASSAULT\" in x[9])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Map to (age_group, 1) pairs\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_rdd = aggr_assault_rdd.map(lambda x: (age_group_from_age(x[11]), 1))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Aggregate counts per age group and sort descending by count\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_rdd = age_group_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Collect and print results and execution time\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "age_group_counts_sorted_rdd = age_group_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "results = age_group_counts_sorted_rdd.collect()\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nExecution time (RDD CSV-safe): {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4db1d0-caa7-44cc-bc5a-09380fe666d6",
   "metadata": {},
   "source": [
    "# QUERY 2\n",
    "Goal:\n",
    "For each year in the LA crime dataset, compute:\n",
    "\n",
    "- The number of victims per Vict Descent group\n",
    "\n",
    "- The percentage of that group relative to the total victims in the same year\n",
    "\n",
    "- Keep only the top 3 Vict Descent groups per year (by victim count)\n",
    "\n",
    "Two implementations:\n",
    "\n",
    "1. DataFrame API (with window functions)\n",
    "\n",
    "2. SQL API (using temp views and SQL window functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b3659-aaa6-4961-994f-3917d709a290",
   "metadata": {},
   "source": [
    "## QUERY 2 - DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10660da-d078-4865-a060-7d869aaa908d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>306</td><td>application_1765289937462_0303</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0303/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0303_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311b481671f44f1d823711b7abbe5b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14831ae335f24d4588df3c5dd90ef921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+------------+\n",
      "|year|        Vict Descent|victim_count|pct_of_total|\n",
      "+----+--------------------+------------+------------+\n",
      "|2010|Hispanic/Latin/Me...|       73558|       35.14|\n",
      "|2010|               White|       53835|       25.72|\n",
      "|2010|               Black|       33937|       16.21|\n",
      "|2011|Hispanic/Latin/Me...|       70845|       35.26|\n",
      "|2011|               White|       51219|       25.49|\n",
      "|2011|               Black|       32579|       16.22|\n",
      "|2012|Hispanic/Latin/Me...|       70338|       34.85|\n",
      "|2012|               White|       51839|       25.68|\n",
      "|2012|               Black|       33572|       16.63|\n",
      "|2013|Hispanic/Latin/Me...|       66741|        34.6|\n",
      "|2013|               White|       48453|       25.12|\n",
      "|2013|               Black|       31975|       16.58|\n",
      "|2014|Hispanic/Latin/Me...|       68763|        35.1|\n",
      "|2014|               White|       47531|       24.27|\n",
      "|2014|               Black|       32952|       16.82|\n",
      "|2015|Hispanic/Latin/Me...|       55978|       33.31|\n",
      "|2015|               White|       44102|       26.24|\n",
      "|2015|               Black|       26510|       15.77|\n",
      "|2016|Hispanic/Latin/Me...|       99135|       34.93|\n",
      "|2016|               White|       63760|       22.47|\n",
      "|2016|               Black|       42449|       14.96|\n",
      "|2017|Hispanic/Latin/Me...|       78308|       33.79|\n",
      "|2017|               White|       52744|       22.76|\n",
      "|2017|               Black|       34713|       14.98|\n",
      "|2018|Hispanic/Latin/Me...|       75958|       33.06|\n",
      "|2018|               White|       52233|       22.73|\n",
      "|2018|               Black|       35340|       15.38|\n",
      "|2019|Hispanic/Latin/Me...|       72458|        33.1|\n",
      "|2019|               White|       48863|       22.32|\n",
      "|2019|               Black|       33157|       15.15|\n",
      "|2020|Hispanic/Latin/Me...|       61606|       30.83|\n",
      "|2020|               White|       42638|       21.34|\n",
      "|2020|               Black|       28785|        14.4|\n",
      "|2021|Hispanic/Latin/Me...|       63676|       30.34|\n",
      "|2021|               White|       44523|       21.21|\n",
      "|2021|               Black|       30173|       14.38|\n",
      "|2022|Hispanic/Latin/Me...|       73111|       31.08|\n",
      "|2022|               White|       46695|       19.85|\n",
      "|2022|               Black|       34634|       14.72|\n",
      "|2023|Hispanic/Latin/Me...|       69401|       29.87|\n",
      "|2023|               White|       44615|        19.2|\n",
      "|2023|                NULL|       31497|       13.56|\n",
      "|2024|                NULL|       29204|       22.89|\n",
      "|2024|Hispanic/Latin/Me...|       28576|        22.4|\n",
      "|2024|               White|       22958|        18.0|\n",
      "|2025|Hispanic/Latin/Me...|          34|       35.05|\n",
      "|2025|             Unknown|          24|       24.74|\n",
      "|2025|               White|          13|        13.4|\n",
      "+----+--------------------+------------+------------+\n",
      "\n",
      "\n",
      " Overall_time: 20.89"
     ]
    }
   ],
   "source": [
    "## Query 2 – DataFrame API\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Crime Data - Query2 (DataFrame API)\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema for main crime dataset\n",
    "# ---------------------------------------------------------------------\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema for race/ethnicity codes (RE_codes.csv)\n",
    "# ---------------------------------------------------------------------\n",
    "race_schema = StructType([\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Start execution timer\n",
    "# ---------------------------------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load crime CSV files from S3\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load race/ethnicity code mapping (Vict Descent -> Vict Descent Full)\n",
    "# ---------------------------------------------------------------------\n",
    "race_codes_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\",\n",
    "    header=True,\n",
    "    schema=race_schema\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Extract year from DATE OCC\n",
    "# ---------------------------------------------------------------------\n",
    "crime_with_year = crime_df.withColumn(\n",
    "    \"year\",\n",
    "    F.year(F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Count victims by (year, Vict Descent code)\n",
    "# ---------------------------------------------------------------------\n",
    "yearly_counts = (\n",
    "    crime_with_year\n",
    "    .groupBy(\"year\", \"Vict Descent\")\n",
    "    .agg(F.count(\"*\").alias(\"victim_count\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Join with race/ethnicity mapping to get full description\n",
    "# ---------------------------------------------------------------------\n",
    "yearly_counts_full = (\n",
    "    yearly_counts\n",
    "    .join(race_codes_df, on=\"Vict Descent\", how=\"left\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Compute total victims per year\n",
    "# ---------------------------------------------------------------------\n",
    "yearly_totals = (\n",
    "    yearly_counts_full\n",
    "    .groupBy(\"year\")\n",
    "    .agg(F.sum(\"victim_count\").alias(\"total\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Join counts with totals and compute percentage\n",
    "# ---------------------------------------------------------------------\n",
    "joined_df = (\n",
    "    yearly_counts_full\n",
    "    .join(yearly_totals, on=\"year\")\n",
    "    .withColumn(\n",
    "        \"pct_of_total\",\n",
    "        F.round(F.col(\"victim_count\") / F.col(\"total\") * 100, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Window: rank descent groups per year by victim_count (descending)\n",
    "# ---------------------------------------------------------------------\n",
    "windowSpec = Window.partitionBy(\"year\").orderBy(F.col(\"victim_count\").desc())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Keep top 3 groups per year, output full description\n",
    "# ---------------------------------------------------------------------\n",
    "final_df = (\n",
    "    joined_df\n",
    "    .withColumn(\"rank\", F.row_number().over(windowSpec))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    "    .select(\n",
    "        \"year\",\n",
    "        F.col(\"Vict Descent Full\").alias(\"Vict Descent\"),\n",
    "        \"victim_count\",\n",
    "        \"pct_of_total\"\n",
    "    )\n",
    "    .orderBy(\"year\", F.col(\"victim_count\").desc())\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Show results and execution time\n",
    "# ---------------------------------------------------------------------\n",
    "final_df.show(48)\n",
    "\n",
    "end = time.time()\n",
    "overall_time = end - start\n",
    "\n",
    "print(f\"\\n Overall_time: {overall_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169578a-30da-42ae-a0ed-85b3ee822a70",
   "metadata": {},
   "source": [
    "## Query 2 – SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129239d3-1f63-4430-8c7c-5c7ef0b92d9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>310</td><td>application_1765289937462_0307</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0307/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-55.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0307_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652bd318b767427eb2d99827dbb83d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d3bf489b774bd097742ffb69c898a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+------------+\n",
      "|year|        Vict Descent|victim_count|pct_of_total|\n",
      "+----+--------------------+------------+------------+\n",
      "|2010|Hispanic/Latin/Me...|       73558|       35.14|\n",
      "|2010|               White|       53835|       25.72|\n",
      "|2010|               Black|       33937|       16.21|\n",
      "|2011|Hispanic/Latin/Me...|       70845|       35.26|\n",
      "|2011|               White|       51219|       25.49|\n",
      "|2011|               Black|       32579|       16.22|\n",
      "|2012|Hispanic/Latin/Me...|       70338|       34.85|\n",
      "|2012|               White|       51839|       25.68|\n",
      "|2012|               Black|       33572|       16.63|\n",
      "|2013|Hispanic/Latin/Me...|       66741|       34.60|\n",
      "|2013|               White|       48453|       25.12|\n",
      "|2013|               Black|       31975|       16.58|\n",
      "|2014|Hispanic/Latin/Me...|       68763|       35.10|\n",
      "|2014|               White|       47531|       24.27|\n",
      "|2014|               Black|       32952|       16.82|\n",
      "|2015|Hispanic/Latin/Me...|       55978|       33.31|\n",
      "|2015|               White|       44102|       26.24|\n",
      "|2015|               Black|       26510|       15.77|\n",
      "|2016|Hispanic/Latin/Me...|       99135|       34.93|\n",
      "|2016|               White|       63760|       22.47|\n",
      "|2016|               Black|       42449|       14.96|\n",
      "|2017|Hispanic/Latin/Me...|       78308|       33.79|\n",
      "|2017|               White|       52744|       22.76|\n",
      "|2017|               Black|       34713|       14.98|\n",
      "|2018|Hispanic/Latin/Me...|       75958|       33.06|\n",
      "|2018|               White|       52233|       22.73|\n",
      "|2018|               Black|       35340|       15.38|\n",
      "|2019|Hispanic/Latin/Me...|       72458|       33.10|\n",
      "|2019|               White|       48863|       22.32|\n",
      "|2019|               Black|       33157|       15.15|\n",
      "|2020|Hispanic/Latin/Me...|       61606|       30.83|\n",
      "|2020|               White|       42638|       21.34|\n",
      "|2020|               Black|       28785|       14.40|\n",
      "|2021|Hispanic/Latin/Me...|       63676|       30.34|\n",
      "|2021|               White|       44523|       21.21|\n",
      "|2021|               Black|       30173|       14.38|\n",
      "|2022|Hispanic/Latin/Me...|       73111|       31.08|\n",
      "|2022|               White|       46695|       19.85|\n",
      "|2022|               Black|       34634|       14.72|\n",
      "|2023|Hispanic/Latin/Me...|       69401|       29.87|\n",
      "|2023|               White|       44615|       19.20|\n",
      "|2024|Hispanic/Latin/Me...|       28576|       22.40|\n",
      "|2024|               White|       22958|       18.00|\n",
      "|2025|Hispanic/Latin/Me...|          34|       35.05|\n",
      "|2025|             Unknown|          24|       24.74|\n",
      "+----+--------------------+------------+------------+\n",
      "\n",
      "\n",
      "Overall time: 30.21"
     ]
    }
   ],
   "source": [
    "## Query 2 – SQL API (with Vict Descent full description)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Crime Data - Query2 (SQL API)\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema for main crime dataset\n",
    "# ---------------------------------------------------------------------\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema for race/ethnicity codes\n",
    "# ---------------------------------------------------------------------\n",
    "race_schema = StructType([\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Start execution timer\n",
    "# ---------------------------------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load crime CSV files\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load race/ethnicity code mapping and register as a view\n",
    "# ---------------------------------------------------------------------\n",
    "race_codes_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\",\n",
    "    header=True,\n",
    "    schema=race_schema\n",
    ")\n",
    "\n",
    "race_codes_df.createOrReplaceTempView(\"race_codes\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Register main crime DataFrame as a temporary view\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Add year from DATE OCC and create a new view\n",
    "# ---------------------------------------------------------------------\n",
    "crime_with_year = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           YEAR(TO_TIMESTAMP(`DATE OCC`, 'yyyy MMM dd hh:mm:ss a')) AS year\n",
    "    FROM crime\n",
    "\"\"\")\n",
    "crime_with_year.createOrReplaceTempView(\"crime_with_year\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Victim counts by (year, Vict Descent code)\n",
    "# ---------------------------------------------------------------------\n",
    "yearly_counts = spark.sql(\"\"\"\n",
    "    SELECT year,\n",
    "           `Vict Descent`,\n",
    "           COUNT(*) AS victim_count\n",
    "    FROM crime_with_year\n",
    "    GROUP BY year, `Vict Descent`\n",
    "\"\"\")\n",
    "yearly_counts.createOrReplaceTempView(\"yearly_counts\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Total victims per year\n",
    "# ---------------------------------------------------------------------\n",
    "yearly_totals = spark.sql(\"\"\"\n",
    "    SELECT year,\n",
    "           SUM(victim_count) AS total\n",
    "    FROM yearly_counts\n",
    "    GROUP BY year\n",
    "\"\"\")\n",
    "yearly_totals.createOrReplaceTempView(\"yearly_totals\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Final query:\n",
    "# - Join yearly_counts with yearly_totals and race_codes\n",
    "# - Compute percentage per group\n",
    "# - Rank groups by victim_count per year\n",
    "# - Keep only top 3\n",
    "# - Output full Vict Descent description\n",
    "# ---------------------------------------------------------------------\n",
    "final_query = \"\"\"\n",
    "SELECT year,\n",
    "       rc.`Vict Descent Full` AS `Vict Descent`,\n",
    "       victim_count,\n",
    "       pct_of_total\n",
    "FROM (\n",
    "    SELECT \n",
    "        yc.year,\n",
    "        yc.`Vict Descent`,\n",
    "        yc.victim_count,\n",
    "        ROUND(100.0 * yc.victim_count / yt.total, 2) AS pct_of_total,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY yc.year\n",
    "            ORDER BY yc.victim_count DESC\n",
    "        ) AS rank\n",
    "    FROM yearly_counts yc\n",
    "    JOIN yearly_totals yt\n",
    "      ON yc.year = yt.year\n",
    ") AS ranked\n",
    "JOIN race_codes rc\n",
    "  ON ranked.`Vict Descent` = rc.`Vict Descent`\n",
    "WHERE rank <= 3\n",
    "ORDER BY year, rank\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(final_query)\n",
    "results.show(48)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Print total execution time\n",
    "# ---------------------------------------------------------------------\n",
    "end = time.time()\n",
    "overall_time = end - start\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8b2c9-eb90-4286-8e2b-1d9663059e1e",
   "metadata": {},
   "source": [
    "# Query 3 – MO code frequency and join strategy comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35284efb-1eea-4dce-9d25-7f882980d4b9",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "1. Extract individual MO codes from the crime dataset (Mocodes column).\n",
    "\n",
    "2. Join them with an external MO code dictionary (MO_codes.txt) that maps each MO code to a textual description.\n",
    "\n",
    "3. Compute the frequency of each MO code and show the description and count, sorted by descending frequency.\n",
    "\n",
    "4. Compare different join strategies (default, broadcast, sort-merge, shuffle hash, shuffle replicate NL).\n",
    "\n",
    "5. Provide an RDD API implementation of the same logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f529a-4110-43fb-9c20-dd80630b769e",
   "metadata": {},
   "source": [
    "## 3.1 DataFrame API – Default join strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b0fdb9-7f55-4e90-8477-e1a69a6ce912",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>119</td><td>application_1761923966900_0134</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0134/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-146.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0134_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f28a953e9f44ac7ad9aefad2d63793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c39afb36ae4da9a774bcccdf831ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [MO_code])\n",
      ":- Filter NOT (MO_code#182 = )\n",
      ":  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      ":     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      ":        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      ":           +- Union false, false\n",
      ":              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      ":              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "+- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "   +- Relation [value#213] text\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "MO_code: string, DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, ... 7 more fields\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      "   :     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :           +- Union false, false\n",
      "   :              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Relation [value#213] text\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :     +- Union false, false\n",
      "   :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "   :        :     +- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "   :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "   +- BroadcastHashJoin [MO_code#182], [MO_code#215], LeftOuter, BuildRight, false\n",
      "      :- Filter NOT (MO_code#182 = )\n",
      "      :  +- Generate explode(MO_array#151), [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], false, [MO_code#182]\n",
      "      :     +- Union\n",
      "      :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "      :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "      :        :     +- FileScan csv [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#10,  , -1), true) > 0), isnotnull(split(Mocodes#10,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "      :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "      :              +- FileScan csv [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#77,  , -1), true) > 0), isnotnull(split(Mocodes#77,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=43]\n",
      "         +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "            +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "               +- FileScan text [value#213] Batched: false, DataFilters: [NOT (split(value#213,  , 2)[0] = ), isnotnull(split(value#213,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================\n",
      "\n",
      "+--------------------+-------+-------+\n",
      "|      MO_description|MO_code|   freq|\n",
      "+--------------------+-------+-------+\n",
      "|Removes vict prop...|   0344|1002900|\n",
      "|            Stranger|   1822| 548422|\n",
      "|   Hit-Hit w/ weapon|   0416| 404773|\n",
      "|          Vandalized|   0329| 377536|\n",
      "| Victim knew Suspect|   0913| 278618|\n",
      "|   Domestic violence|   2000| 256188|\n",
      "|    Vehicle involved|   1300| 219082|\n",
      "|          Force used|   0400| 213165|\n",
      "|Evidence Booked (...|   1402| 177470|\n",
      "|             Smashed|   1609| 131229|\n",
      "|   Susp uses vehicle|   1309| 122108|\n",
      "|Victim was aged (...|   1202| 120238|\n",
      "|    Took merchandise|   0325| 120159|\n",
      "|Susp is/was curre...|   1814| 118073|\n",
      "|              Pushed|   0444| 116763|\n",
      "|  Other MO (see rpt)|   1501| 115589|\n",
      "|       Breaks window|   1307| 113609|\n",
      "|   Brandishes weapon|   0334| 105665|\n",
      "|Suspect is homele...|   2004|  93426|\n",
      "|        Intimidation|   0432|  83562|\n",
      "|Multi-susps overw...|   0342|  81230|\n",
      "|    Threaten to kill|   0421|  81036|\n",
      "|               Gangs|   0906|  78910|\n",
      "|Vict knocked to g...|   0429|  77442|\n",
      "|Susp is/was curre...|   1813|  76688|\n",
      "|Takes vict's iden...|   0377|  75529|\n",
      "|           Ransacked|   0321|  72398|\n",
      "|       Open/unlocked|   1606|  71874|\n",
      "|             Grabbed|   0448|  70142|\n",
      "|               Pried|   1607|  68012|\n",
      "|        Forces Entry|   0358|  67791|\n",
      "|Victim was a student|   1251|  67324|\n",
      "|Took Victim's clo...|   0352|  64413|\n",
      "|  Suspect swung fist|   0446|  62226|\n",
      "| Suspect Impersonate|   0100|  62021|\n",
      "|Unauthorized use ...|   0930|  54810|\n",
      "|              Kicked|   0417|  49317|\n",
      "|Threaten to harm ...|   0443|  49182|\n",
      "|        Bodily Force|   1601|  46968|\n",
      "|      Profanity Used|   0319|  45655|\n",
      "|         Photographs|   1414|  44327|\n",
      "|           Aimed gun|   0302|  44178|\n",
      "|Victim was Homele...|   1218|  42072|\n",
      "|Suspect removed p...|   0385|  41926|\n",
      "|Victim is 14 year...|   1259|  41809|\n",
      "|Suspect wore hood...|   0216|  38316|\n",
      "|         Shots Fired|   1100|  37298|\n",
      "|Choked/uses choke...|   0408|  37019|\n",
      "|   Attacks from rear|   0305|  34614|\n",
      "|   Victim in vehicle|   1310|  34255|\n",
      "|        Co-habitants|   1243|  33283|\n",
      "|Victim is 6 years...|   1258|  30566|\n",
      "|Restraining order...|   2038|  30241|\n",
      "|Landlord/Tenant/N...|   0603|  29957|\n",
      "|Suspect intoxicat...|   2002|  29777|\n",
      "|            Customer|   0104|  29527|\n",
      "|Suspect threw obj...|   0447|  29421|\n",
      "| Pulled victims hair|   0419|  29175|\n",
      "|Suspect swung weapon|   0445|  28702|\n",
      "|Public Transit (M...|   0910|  27000|\n",
      "|Forged or Fraudul...|   0928|  26810|\n",
      "|        Cutting Tool|   1602|  26107|\n",
      "|       Demands money|   0337|  25726|\n",
      "|             Cap/hat|   0202|  25628|\n",
      "|         Cut/stabbed|   0411|  25597|\n",
      "|Susp takes UPS, F...|   0394|  25369|\n",
      "|Demanded property...|   0355|  25329|\n",
      "|Video surveillanc...|   1420|  25175|\n",
      "| Suspect is neighbor|   0361|  24253|\n",
      "|Snatch property a...|   0346|  23955|\n",
      "|             Touched|   0522|  23954|\n",
      "|     Bullets/Casings|   1407|  23408|\n",
      "|              Family|   0602|  23159|\n",
      "|Suspect forced wa...|   1302|  23045|\n",
      "|Unauthorized use ...|   0929|  21610|\n",
      "|THEFT: Trick or D...|   0701|  21149|\n",
      "|        Acquaintance|   1817|  21045|\n",
      "|Under influence d...|   1206|  20668|\n",
      "|ATM Theft with PI...|   0922|  18583|\n",
      "|           Vict shot|   0430|  16895|\n",
      "|Victim left prope...|   2032|  16863|\n",
      "|Suspect is Other ...|   0360|  16325|\n",
      "|Stolen/Forged Che...|   0923|  16306|\n",
      "|   LA Police Officer|   1212|  15869|\n",
      "|    Sex related acts|   0500|  15613|\n",
      "|            Graffiti|   0311|  15529|\n",
      "|Suspect is Victim...|   0561|  15259|\n",
      "|Gang affiliation ...|   0371|  15199|\n",
      "|Harrassing E-Mail...|   1906|  15085|\n",
      "|Missing Clothing/...|   1221|  14545|\n",
      "|Clerk/Employer/Owner|   1236|  14482|\n",
      "|  Lock slip/key/pick|   1605|  13327|\n",
      "|  Actual Intercourse|   0527|  13294|\n",
      "| Victim's Employment|   0917|  13292|\n",
      "|                 Bit|   0401|  13049|\n",
      "|            Business|   0601|  12641|\n",
      "|Susp is/was roommate|   1810|  12437|\n",
      "|         Riding bike|   0345|  12104|\n",
      "|Suspect spits on ...|   0356|  12100|\n",
      "|              Spouse|   1241|  11889|\n",
      "|Victim was securi...|   1266|  11714|\n",
      "|           Hot Prowl|   0314|  11673|\n",
      "|Cut lock (to bicy...|   0397|  11309|\n",
      "|               BUNCO|   0800|  11083|\n",
      "|             Removed|   1608|  10988|\n",
      "|       Estes Robbery|   0945|  10951|\n",
      "|Suspect was Juvenile|   2024|  10831|\n",
      "|      Brings own bag|   0378|  10761|\n",
      "|Suspect is Victim...|   0554|  10759|\n",
      "|Suspect is/was kn...|   1816|  10437|\n",
      "|         Used driver|   0326|   9971|\n",
      "|Suspect shot at v...|   0450|   9856|\n",
      "|              Spouse|   1821|   9824|\n",
      "|              Friend|   1820|   9809|\n",
      "|Suspect is Victim...|   0552|   9593|\n",
      "|Victim is Newborn...|   1257|   9497|\n",
      "|Sprayed with chem...|   0431|   9466|\n",
      "|Suspect was Aged ...|   2021|   9204|\n",
      "|Suspect exits veh...|   1313|   8802|\n",
      "|Professional (doc...|   1210|   8668|\n",
      "|    Gun in waistband|   0312|   8556|\n",
      "|Suspect speaks sp...|   0369|   8468|\n",
      "|Suspect follows v...|   0340|   8387|\n",
      "|Put a weapon to body|   0449|   8327|\n",
      "|Suspect is Victim...|   0553|   8046|\n",
      "|Stolen/Forged Che...|   0924|   7924|\n",
      "|Threatening E-mai...|   1912|   7882|\n",
      "|Threaten Victims ...|   0422|   7643|\n",
      "|            Rape Kit|   1415|   7407|\n",
      "|           Road Rage|   0919|   7300|\n",
      "|Frisks victim/pat...|   0370|   7128|\n",
      "|Firearm booked as...|   1419|   6989|\n",
      "|Suspect takes car...|   0354|   6764|\n",
      "|Misc. Stolen/Forg...|   0935|   6709|\n",
      "|Susp is/was victi...|   1803|   6643|\n",
      "|                Mask|   0209|   6532|\n",
      "|Suspect was 5150/...|   2003|   6523|\n",
      "|    Susp was student|   1815|   6503|\n",
      "|   Indecent Exposure|   0529|   6290|\n",
      "|        Chain snatch|   0336|   6227|\n",
      "|        Used lookout|   0327|   6198|\n",
      "|Victim was asleep...|   2034|   6051|\n",
      "|Vehicle occupant/...|   1240|   5953|\n",
      "|Suspect is Victim...|   0562|   5653|\n",
      "|Narcotics (Buy-Se...|   0907|   5589|\n",
      "|Put hand, finger ...|   0515|   5537|\n",
      "|Suspect follows v...|   1312|   5533|\n",
      "|       Fondle victim|   0503|   5407|\n",
      "|Suspect removes v...|   0537|   5293|\n",
      "|Suspect touches v...|   0551|   5277|\n",
      "|Kissed victims bo...|   0510|   5267|\n",
      "|Victim refused to...|   2033|   5169|\n",
      "|Suspect removed p...|   0386|   5165|\n",
      "|    Suspect on drugs|   2001|   5099|\n",
      "|    Hatred/Prejudice|   0903|   5016|\n",
      "|Suspect was Repea...|   2028|   5006|\n",
      "|Victims vehicle t...|   0330|   4894|\n",
      "|      Consentual Sex|   0533|   4771|\n",
      "|Made unusual stat...|   0359|   4700|\n",
      "|Traffic Accident/...|   0605|   4613|\n",
      "|Forced theft of v...|   0916|   4581|\n",
      "|Takes money from ...|   0324|   4567|\n",
      "|Punched/Pulled Do...|   1612|   4384|\n",
      "|Susp is/was victi...|   1802|   4374|\n",
      "|           Kidnapped|   0418|   4262|\n",
      "|          Takes mail|   0349|   4241|\n",
      "|          Masturbate|   0528|   3930|\n",
      "|             Clothes|   1409|   3927|\n",
      "|Suspect is Victim...|   0555|   3849|\n",
      "|              Gloves|   0206|   3705|\n",
      "|               Money|   1019|   3346|\n",
      "|            Pregnant|   1276|   3322|\n",
      "|Forced to orally ...|   0507|   3287|\n",
      "|   Drive-by shooting|   0309|   3216|\n",
      "|              Sodomy|   0519|   3105|\n",
      "|Vacant Residence/...|   1275|   3070|\n",
      "|              Parent|   1242|   2950|\n",
      "|Suspect made sexu...|   0531|   2942|\n",
      "|Brief encounter/Date|   1823|   2939|\n",
      "|           Classmate|   1824|   2854|\n",
      "|        Fingerprints|   1403|   2818|\n",
      "| Victim was customer|   1237|   2752|\n",
      "|Suspect removes o...|   0536|   2682|\n",
      "|Orally copulated ...|   0512|   2603|\n",
      "|Suspect was passe...|   2052|   2589|\n",
      "|            Searched|   0420|   2585|\n",
      "|Suspect is Victim...|   0558|   2541|\n",
      "|Suspect is Victim...|   0556|   2535|\n",
      "|        Purse snatch|   0390|   2492|\n",
      "|Victim was physic...|   1238|   2456|\n",
      "|Victim was gang m...|   1270|   2449|\n",
      "|Victim of crime p...|   1203|   2440|\n",
      "|Suspect meets vic...|   1913|   2410|\n",
      "|Reached climax/ej...|   0516|   2407|\n",
      "|         Riding bike|   1223|   2379|\n",
      "|Makes victim give...|   0316|   2374|\n",
      "|         Stalks vict|   0347|   2338|\n",
      "|    Distracts Victim|   0380|   2308|\n",
      "|    Weapon Concealed|   0353|   2305|\n",
      "|Victim was passen...|   2049|   2285|\n",
      "|Jumped counter/go...|   0315|   2284|\n",
      "|Victim removed fr...|   1311|   2282|\n",
      "|Suspect was passe...|   2051|   2272|\n",
      "|Auction Fraud/eBa...|   1900|   2260|\n",
      "|Elder Abuse/Physical|   1701|   2183|\n",
      "|Home under constr...|   1267|   2166|\n",
      "|Suspect undressed...|   0532|   2155|\n",
      "|Suspects offers/s...|   1000|   2123|\n",
      "|            Ski mask|   0213|   1950|\n",
      "|               Sales|   1216|   1946|\n",
      "|Victim was 5150/M...|   1268|   1918|\n",
      "|    Indistinctive MO|   9999|   1901|\n",
      "|         Knock-knock|   0389|   1893|\n",
      "| On Vacation/Tourist|   1205|   1847|\n",
      "|        Prostitution|   0908|   1837|\n",
      "|Victim forced int...|   1305|   1829|\n",
      "|Victim targeted b...|   2055|   1796|\n",
      "|Victim was a stre...|   1252|   1793|\n",
      "|Victim was Uber/L...|   1278|   1788|\n",
      "|Removes cash regi...|   0375|   1729|\n",
      "|      Victim was gay|   1222|   1693|\n",
      "|Victim was on tra...|   2048|   1677|\n",
      "|Stolen/Forged Che...|   0925|   1628|\n",
      "|Victim was passen...|   2050|   1619|\n",
      "|Met online/Chat R...|   0940|   1608|\n",
      "|Counterfeit or fo...|   0931|   1578|\n",
      "|        Other Felony|   0914|   1556|\n",
      "|Male Victim of se...|   0545|   1507|\n",
      "|              Hugged|   0509|   1490|\n",
      "|Removed money/pro...|   0382|   1466|\n",
      "|Victim paid by wi...|   1916|   1460|\n",
      "|Susp is/was mothe...|   1801|   1443|\n",
      "|Susp is/was victi...|   1804|   1413|\n",
      "| Cover mouth w/hands|   0409|   1408|\n",
      "|  Suspect apologizes|   0381|   1407|\n",
      "|Bomb Threat, no bomb|   0404|   1406|\n",
      "|Crime on upper floor|   0306|   1401|\n",
      "|Susp is/was victi...|   1811|   1361|\n",
      "|       Home invasion|   0363|   1357|\n",
      "|Caretaker/care-gi...|   1818|   1348|\n",
      "|Forged or Telepho...|   0926|   1343|\n",
      "|Victim was injure...|   2047|   1330|\n",
      "|     Demands jewelry|   0308|   1322|\n",
      "|Weapon (other tha...|   0387|   1310|\n",
      "|Unauthorized acce...|   1914|   1295|\n",
      "|        Handkerchief|   0207|   1290|\n",
      "|Victim exiting th...|   1317|   1254|\n",
      "|               Paint|   1413|   1208|\n",
      "|  Internet Extortion|   1915|   1199|\n",
      "|Ate/drank on prem...|   0304|   1189|\n",
      "|Raped while uncon...|   1262|   1175|\n",
      "|Teenager(Use if v...|   1217|   1153|\n",
      "|Suspect was Uber/...|   2014|   1141|\n",
      "|Solicited/offered...|   0520|   1140|\n",
      "|Victim parking, g...|   1306|   1128|\n",
      "|   Human Trafficking|   0943|   1108|\n",
      "|                Ride|   1022|   1107|\n",
      "|        Bar Customer|   1234|   1105|\n",
      "|             Drugged|   0413|   1105|\n",
      "|     Suspect was gay|   2007|   1071|\n",
      "|           Cigarette|   1008|   1071|\n",
      "|              Patron|   1256|   1066|\n",
      "|Use the phone or ...|   1026|   1059|\n",
      "|          Prostitute|   1215|   1050|\n",
      "|     Utilized Condom|   0526|   1044|\n",
      "|         Pornography|   0514|   1034|\n",
      "|         Taxi Driver|   1231|   1028|\n",
      "|          Bus Driver|   1253|    996|\n",
      "|Victim was mental...|   1261|    992|\n",
      "|          LA Fireman|   1213|    984|\n",
      "|Victim was Pistol...|   1281|    962|\n",
      "|            Tunneled|   1610|    959|\n",
      "|      Brutal Assault|   0406|    957|\n",
      "|              Police|   0112|    925|\n",
      "|   Disabled Security|   0351|    915|\n",
      "|    Opening business|   1247|    913|\n",
      "|Susp is/was fello...|   1805|    910|\n",
      "|Bias: AntI-Black ...|   1514|    910|\n",
      "|Gang signs/threw ...|   0374|    908|\n",
      "| Makes vict lie down|   0341|    904|\n",
      "|Disables video ca...|   0339|    903|\n",
      "|     Delivery person|   1227|    900|\n",
      "|  Victim inside tent|   2060|    874|\n",
      "|Susp ejaculated o...|   0501|    871|\n",
      "|        Racial slurs|   2035|    867|\n",
      "|       Burned Victim|   0407|    861|\n",
      "|         Twisted arm|   0426|    832|\n",
      "|           Narcotics|   1020|    814|\n",
      "|Victim was Law En...|   1271|    800|\n",
      "|        Blood Stains|   1401|    787|\n",
      "|Hate-related lang...|   2036|    773|\n",
      "|Suspect fondles self|   0538|    770|\n",
      "|    Cannabis related|   2018|    761|\n",
      "|Victim targeted b...|   2054|    736|\n",
      "|         Information|   1017|    729|\n",
      "|Stopped victim ve...|   1304|    727|\n",
      "|   Common-law Spouse|   1819|    715|\n",
      "|       Hate Incident|   0921|    708|\n",
      "|Party/Flier party...|   0942|    708|\n",
      "|Suspect was trans...|   2006|    707|\n",
      "|Drives by and sna...|   1308|    702|\n",
      "|  Disables Telephone|   0338|    701|\n",
      "|Victim targeted b...|   2053|    676|\n",
      "|Home was being fu...|   1273|    672|\n",
      "| Photographed victim|   0513|    645|\n",
      "|Suspect speaks fo...|   0368|    642|\n",
      "|Harassment via So...|   2041|    635|\n",
      "|Hit victim prior,...|   0508|    628|\n",
      "|Orders victim to ...|   0542|    625|\n",
      "|             Revenge|   0911|    624|\n",
      "|Victim entering t...|   1316|    617|\n",
      "|Bias: Anti-Gay (M...|   1536|    617|\n",
      "|Suspect puts fing...|   0540|    609|\n",
      "|   Bias: Anti-Jewish|   1528|    609|\n",
      "|                 Bag|   0201|    608|\n",
      "|Threats via Socia...|   2040|    603|\n",
      "|                NULL|   0853|    601|\n",
      "|Physicall disable...|   0373|    600|\n",
      "|         Transgender|   1239|    590|\n",
      "|Suspect follows v...|   1318|    587|\n",
      "|Photographed vict...|   0372|    575|\n",
      "|          Shaved Key|   1611|    574|\n",
      "|Susp is/was rival...|   1809|    569|\n",
      "|      Cases location|   0335|    558|\n",
      "|Victim used profa...|   2029|    554|\n",
      "|        Banking, ATM|   1214|    548|\n",
      "|Susp is/was foste...|   1812|    547|\n",
      "|          Directions|   1009|    545|\n",
      "|     Stop sign/light|   1225|    543|\n",
      "|   Mailbox Vandalism|   0332|    539|\n",
      "|  Defecated/urinated|   0307|    534|\n",
      "|Roof access (remo...|   0398|    519|\n",
      "|              Liquor|   1018|    518|\n",
      "|Bias: Anti-Hispan...|   1516|    509|\n",
      "|Fraudulent or for...|   0927|    505|\n",
      "|Suspect uses whee...|   2005|    496|\n",
      "|Elder Abuse/Finan...|   1702|    495|\n",
      "|Forced to fondle ...|   0505|    490|\n",
      "|Suspect is Victim...|   0559|    489|\n",
      "|Covered victim's ...|   0410|    480|\n",
      "|DWP/Gas Company/U...|   0121|    475|\n",
      "|    Nude/partly nude|   0212|    466|\n",
      "|Credit Card Fraud...|   1902|    460|\n",
      "|Tore clothes off ...|   0424|    455|\n",
      "|Suspect licks victim|   0550|    453|\n",
      "|              Saliva|   1416|    453|\n",
      "|Social Security/M...|   0120|    450|\n",
      "|Victim loading ve...|   1314|    449|\n",
      "|Suspect entered d...|   0383|    447|\n",
      "|            Ambushed|   0303|    446|\n",
      "|Susp is/was other...|   1808|    434|\n",
      "|Cuts or breaks pu...|   0357|    433|\n",
      "|Bomb Threat, Bomb...|   0403|    432|\n",
      "|Suspect in vehicl...|   0534|    429|\n",
      "|             Parolee|   0915|    419|\n",
      "|             Mailbox|   1265|    413|\n",
      "|Temporary/Vacatio...|   2037|    405|\n",
      "|Suspect puts obje...|   0541|    404|\n",
      "|Suspect uses vict...|   0934|    404|\n",
      "|    Takeover robbery|   0365|    401|\n",
      "|                NULL|   0858|    400|\n",
      "|                NULL|   2126|    377|\n",
      "|Smashed display case|   0322|    372|\n",
      "|              Moving|   1204|    371|\n",
      "|Identity Theft vi...|   1908|    368|\n",
      "|Forced to masturb...|   0506|    358|\n",
      "|Suspect was Pregnant|   2027|    357|\n",
      "|            Lock Box|   1604|    353|\n",
      "|Deaf/Hearing Impa...|   1260|    349|\n",
      "|Simulated interco...|   0518|    349|\n",
      "|          Bite Marks|   1408|    349|\n",
      "|Victim unloading ...|   1315|    342|\n",
      "|               Bound|   0405|    340|\n",
      "|                NULL|   0857|    337|\n",
      "|                NULL|   0856|    337|\n",
      "|Vehicle to Vehicl...|   0399|    335|\n",
      "|     Career Criminal|   0918|    332|\n",
      "|Motorized Rental ...|   0947|    329|\n",
      "|Orders vict to re...|   0343|    327|\n",
      "|Suspect removed d...|   0384|    317|\n",
      "|                NULL|   3028|    316|\n",
      "|Ordered vict to o...|   0366|    310|\n",
      "|   Forced to disrobe|   0504|    308|\n",
      "|Cyberstalking (St...|   1903|    303|\n",
      "|    Used demand note|   0391|    303|\n",
      "|                Food|   1013|    302|\n",
      "|     Postal employee|   1230|    302|\n",
      "|  Makes victim kneel|   0433|    296|\n",
      "|                NULL|   3004|    294|\n",
      "|Turns off lights/...|   0379|    294|\n",
      "|Child Pornography...|   1901|    292|\n",
      "|                Nude|   1219|    287|\n",
      "|           Left Note|   1405|    280|\n",
      "|Suspect solicits ...|   1028|    278|\n",
      "|Involved in traff...|   0110|    266|\n",
      "|    Handcuffed/Metal|   0415|    264|\n",
      "|Suspect was Inmat...|   2009|    264|\n",
      "|             Uniform|   0217|    262|\n",
      "|Escaped on (used)...|   0301|    262|\n",
      "|Active Shooter/Ar...|   0442|    261|\n",
      "|Forced victim veh...|   1301|    259|\n",
      "|              Change|   1027|    258|\n",
      "|Suspect uses vict...|   0933|    258|\n",
      "|          Aid victim|   0101|    257|\n",
      "|             Renting|   0113|    256|\n",
      "|Suspect wore disg...|   0200|    256|\n",
      "|Vendor (street or...|   2011|    254|\n",
      "|          Footprints|   1404|    252|\n",
      "|Suspect ejaculate...|   0549|    251|\n",
      "|False Emergency R...|   0392|    243|\n",
      "|          Knob Twist|   1603|    243|\n",
      "|       Repair Person|   0114|    243|\n",
      "|           Assistant|   1004|    238|\n",
      "|Used paper plates...|   0396|    237|\n",
      "|                NULL|   3003|    234|\n",
      "|         Blindfolded|   0402|    230|\n",
      "|           911 Abuse|   0393|    229|\n",
      "|Got victim to wit...|   0310|    225|\n",
      "|Leaving Business ...|   1228|    225|\n",
      "|             Whipped|   0427|    224|\n",
      "|          Employment|   1011|    223|\n",
      "|Orders victim to ...|   0543|    221|\n",
      "|Suspect is Victim...|   0557|    212|\n",
      "|Victim targeted b...|   2058|    211|\n",
      "|Minor solicited f...|   1910|    207|\n",
      "|Tied victim to ob...|   0423|    206|\n",
      "|Telephone/Electri...|   0440|    202|\n",
      "|Suspect was Prost...|   2023|    202|\n",
      "|                NULL|   3101|    197|\n",
      "|  Drink (not liquor)|   1010|    196|\n",
      "|          Tool Marks|   1406|    195|\n",
      "|                 Wig|   0218|    193|\n",
      "|    Gun Shot Residue|   1410|    184|\n",
      "|   Masochism/bondage|   0511|    184|\n",
      "|         Used toilet|   0328|    182|\n",
      "|Suspect is babysi...|   0364|    181|\n",
      "|Bias: Anti-Transg...|   1510|    181|\n",
      "| Pillowcase/suitcase|   0317|    174|\n",
      "|Theft of animal (...|   2019|    172|\n",
      "|Mistreatment of a...|   2020|    172|\n",
      "|Victim was at/lea...|   1272|    171|\n",
      "|                NULL|   3037|    171|\n",
      "|      Murder/Suicide|   0395|    170|\n",
      "|    Closing business|   1248|    166|\n",
      "|Suspect attempts ...|   0362|    165|\n",
      "|                NULL|   2304|    162|\n",
      "|         Suffocation|   0451|    161|\n",
      "|            Salesman|   0117|    160|\n",
      "|Victim used racia...|   2030|    160|\n",
      "|    Bias: Anti-White|   1520|    157|\n",
      "|                NULL|   0851|    157|\n",
      "|                NULL|   0852|    155|\n",
      "|    Bias: Anti-Asian|   1513|    153|\n",
      "|      Partially Nude|   1220|    153|\n",
      "|Theft of computer...|   1911|    152|\n",
      "|        Quiet polite|   0320|    151|\n",
      "|Tape/Electrical e...|   0439|    151|\n",
      "|            Urinated|   0525|    149|\n",
      "|Suspect wore moto...|   0220|    149|\n",
      "|  Smoked on premises|   0323|    146|\n",
      "|             Gardner|   1277|    145|\n",
      "|Catering Truck Op...|   1226|    143|\n",
      "|Clothes of opposi...|   0204|    142|\n",
      "|       Bank, Leaving|   1233|    140|\n",
      "|                NULL|   2201|    139|\n",
      "|             Jewelry|   1412|    138|\n",
      "|   Salesman, Jewelry|   1209|    136|\n",
      "|Flexcuffs/Plastic...|   0437|    134|\n",
      "|      Animal Neglect|   0938|    133|\n",
      "|Tongue or mouth t...|   0521|    133|\n",
      "|Bias: Anti-Other ...|   1519|    133|\n",
      "|                NULL|   4026|    131|\n",
      "|Hate Crime materi...|   1907|    131|\n",
      "|Victim used hate-...|   2031|    131|\n",
      "|                NULL|   2204|    129|\n",
      "|                NULL|   3401|    128|\n",
      "|                NULL|   3701|    128|\n",
      "|           Insurance|   0912|    123|\n",
      "|  Makes victim kneel|   0376|    122|\n",
      "|              Gagged|   0414|    119|\n",
      "|Victim staying at...|   2042|    119|\n",
      "|Suspect is Victim...|   0560|    117|\n",
      "|                NULL|   3026|    117|\n",
      "|     Hid in building|   0313|    113|\n",
      "|Victim was Inmate...|   1274|    112|\n",
      "|     Public Official|   1211|    109|\n",
      "|            Delivery|   0105|    109|\n",
      "|Susp is/was fathe...|   1806|    108|\n",
      "|Bisexual/sexually...|   1235|    108|\n",
      "|Suspect points la...|   0388|    108|\n",
      "|        Rope/Cordage|   0438|    103|\n",
      "|     Seeking someone|   0118|    103|\n",
      "|                NULL|   3030|    101|\n",
      "|                NULL|   3025|    100|\n",
      "|Susp is/was pries...|   1807|     99|\n",
      "|Absent-advertised...|   1201|     99|\n",
      "|      Halloween mask|   0208|     98|\n",
      "|Suspect force vic...|   0547|     97|\n",
      "|      Takeover other|   0348|     97|\n",
      "|Victim targeted b...|   2056|     95|\n",
      "|Unable to get ere...|   0523|     94|\n",
      "|     Aid for vehicle|   1001|     93|\n",
      "|          Disfigured|   0412|     92|\n",
      "|  Political Activity|   0902|     91|\n",
      "|     Unusual clothes|   0215|     91|\n",
      "|Suspect uses vict...|   0932|     90|\n",
      "|Followed Transit ...|   1255|     88|\n",
      "|Bias: Anti-Lesbia...|   1539|     88|\n",
      "|                NULL|   2100|     88|\n",
      "|               Semen|   1417|     86|\n",
      "|Suspect puts hand...|   0539|     84|\n",
      "|                NULL|   0859|     83|\n",
      "|           Amusement|   1002|     82|\n",
      "|Suspect asks mino...|   0535|     82|\n",
      "|            Tortured|   0425|     81|\n",
      "|    Hid in rear seat|   1303|     81|\n",
      "|Victim is owner o...|   2043|     80|\n",
      "|           Gang Feud|   0946|     79|\n",
      "|            Clothing|   0436|     77|\n",
      "|           Inspector|   0109|     77|\n",
      "|                NULL|   0850|     77|\n",
      "|       Prepared exit|   0318|     76|\n",
      "|              Repair|   1021|     75|\n",
      "|      Used lubricant|   0530|     74|\n",
      "|             Earring|   0205|     72|\n",
      "|               Candy|   1007|     71|\n",
      "|               Blind|   0102|     68|\n",
      "|Suspect was MTA B...|   2017|     66|\n",
      "|Suspect staying a...|   2044|     65|\n",
      "|  Bias: Anti-Lesbian|   1538|     64|\n",
      "|                NULL|   2108|     63|\n",
      "|     Mailbox Bombing|   0331|     62|\n",
      "|Suspect damaged p...|   2046|     62|\n",
      "|                NULL|   0855|     62|\n",
      "|Returning stolen ...|   0115|     61|\n",
      "|Cloth (with eyeho...|   0203|     61|\n",
      "|          Contractor|   0122|     61|\n",
      "|Drive-through (no...|   1224|     60|\n",
      "|Suspect was Deaf/...|   2026|     60|\n",
      "|                NULL|   2104|     60|\n",
      "|      Bait Operation|   0944|     60|\n",
      "|Victim was forced...|   1245|     59|\n",
      "|       Ritual/Occult|   0909|     59|\n",
      "|                NULL|   0854|     58|\n",
      "|Shots Fired (Anim...|   1101|     57|\n",
      "|                NULL|   3006|     57|\n",
      "|Concealed victim'...|   0350|     55|\n",
      "|     Organized Crime|   0901|     54|\n",
      "|Suspect was Jaile...|   2010|     53|\n",
      "|          Find a job|   1012|     53|\n",
      "|               Shoes|   0211|     53|\n",
      "|    Making bank drop|   1229|     52|\n",
      "|              Doctor|   0106|     52|\n",
      "|       Sent by owner|   0119|     51|\n",
      "|Destruction of co...|   1905|     51|\n",
      "|   Homeland Security|   0920|     50|\n",
      "|Strike/Labor Trou...|   0904|     50|\n",
      "|Susp instructs vi...|   0546|     49|\n",
      "|       Fetish, Other|   0563|     49|\n",
      "|Used hand held ra...|   0333|     47|\n",
      "|                NULL|   3029|     47|\n",
      "|                NULL|   2303|     46|\n",
      "|Victim was Foster...|   1279|     46|\n",
      "|Orders victim to ...|   0544|     45|\n",
      "|                NULL|   3036|     45|\n",
      "|                NULL|   3034|     45|\n",
      "|       Mustache-Fake|   0219|     44|\n",
      "|Vict rptd sexual ...|   2059|     44|\n",
      "|        Fecal Fetish|   0502|     43|\n",
      "| Physically disabled|   0103|     43|\n",
      "|                NULL|   3035|     43|\n",
      "|               Chain|   0435|     42|\n",
      "|      Train Operator|   1254|     42|\n",
      "|Suspect was Foste...|   2015|     42|\n",
      "|     Bias: Anti-Arab|   1512|     41|\n",
      "|                Gift|   1015|     40|\n",
      "|   Bank, Arriving at|   1232|     39|\n",
      "|Introduction of v...|   1909|     39|\n",
      "|                NULL|   4016|     38|\n",
      "|Sadism/Sexual gra...|   0517|     38|\n",
      "|Victim was forced...|   1244|     38|\n",
      "|Victim was undocu...|   1208|     38|\n",
      "|            Stocking|   0214|     38|\n",
      "|Bias: Anti-Other ...|   1532|     37|\n",
      "|                Wire|   0441|     36|\n",
      "|Victim targeted b...|   2057|     36|\n",
      "|                Hair|   1411|     35|\n",
      "|                NULL|   4010|     35|\n",
      "|          Hitchhiker|   1207|     34|\n",
      "|         Dismembered|   0428|     34|\n",
      "|Bias: Anti-Islami...|   1526|     34|\n",
      "|                NULL|   3011|     33|\n",
      "|      Skeleton/Bones|   1418|     32|\n",
      "|                NULL|   2180|     31|\n",
      "|In possession of ...|   2008|     29|\n",
      "|                NULL|   4015|     29|\n",
      "|Victim was Foster...|   1280|     29|\n",
      "|                NULL|   4025|     29|\n",
      "|   Bed Sheets/Linens|   0434|     28|\n",
      "|                 God|   0107|     27|\n",
      "|Make up (males only)|   0210|     27|\n",
      "|Denial of compute...|   1904|     26|\n",
      "|Bias: Anti-Other ...|   1531|     26|\n",
      "|Suspect was Bisexual|   2025|     25|\n",
      "|Gardener/Tree Tri...|   0123|     24|\n",
      "|                NULL|   4027|     24|\n",
      "|Suspect was costu...|   2012|     24|\n",
      "|                NULL|   2208|     24|\n",
      "|  Bias : Anti-female|   1507|     24|\n",
      "|                Game|   1014|     23|\n",
      "|    Underwear Fetish|   0524|     23|\n",
      "|                NULL|   3008|     23|\n",
      "|Bias: Anti-Multip...|   1517|     22|\n",
      "|                NULL|   4009|     22|\n",
      "|Victim was costum...|   2039|     21|\n",
      "|       Cock Fighting|   0937|     21|\n",
      "|Bias: Anti-Gender...|   1509|     20|\n",
      "|                NULL|   4017|     20|\n",
      "|              Infirm|   0108|     18|\n",
      "|                NULL|   2185|     18|\n",
      "|                NULL|   4021|     17|\n",
      "|                NULL|   2151|     17|\n",
      "|                NULL|   4019|     17|\n",
      "|                NULL|   3032|     16|\n",
      "|                NULL|   2301|     16|\n",
      "|Hold for safekeeping|   1016|     16|\n",
      "|                NULL|   2225|     16|\n",
      "|     Animal Hoarding|   0939|     16|\n",
      "|Bias: Physical di...|   1506|     16|\n",
      "|        Dog Fighting|   0936|     15|\n",
      "|     Terrorist Group|   0905|     15|\n",
      "|                NULL|   3024|     14|\n",
      "|Victim was armore...|   1269|     14|\n",
      "|Reproductive Heal...|   0604|     14|\n",
      "| Bias: Anti-Catholic|   1523|     13|\n",
      "|                NULL|   3104|     13|\n",
      "|Bias: Anti-Citize...|   1515|     13|\n",
      "|                NULL|   2202|     13|\n",
      "|               Train|   1025|     13|\n",
      "|Bias: Mental Disa...|   1505|     13|\n",
      "|                NULL|   2117|     13|\n",
      "|                NULL|   2203|     12|\n",
      "|                NULL|   2200|     12|\n",
      "|                NULL|   3033|     12|\n",
      "|            Pipeline|   1264|     12|\n",
      "|Tour Bus/Van Oper...|   2013|     12|\n",
      "| Bias: Anti-Bisexual|   1535|     12|\n",
      "|                NULL|   2157|     11|\n",
      "|                NULL|   2308|     11|\n",
      "|                NULL|   2215|     11|\n",
      "|                NULL|   3039|     11|\n",
      "|                NULL|   2182|     10|\n",
      "|                NULL|   2106|     10|\n",
      "|                NULL|   2158|     10|\n",
      "|                NULL|   4012|     10|\n",
      "|Bias: Anti-Jehova...|   1527|     10|\n",
      "|                NULL|   4018|     10|\n",
      "|                NULL|   4008|     10|\n",
      "|                NULL|   4005|      9|\n",
      "|                NULL|   2109|      9|\n",
      "|                NULL|   2206|      9|\n",
      "|            Audition|   1005|      8|\n",
      "|Suspect is owner ...|   2045|      8|\n",
      "|                NULL|   2217|      8|\n",
      "|Suspect was Hitch...|   2022|      8|\n",
      "|                NULL|   3009|      8|\n",
      "|                NULL|   2210|      7|\n",
      "|                NULL|   2212|      7|\n",
      "|     Bias: Anti-male|   1508|      7|\n",
      "|                NULL|   2118|      7|\n",
      "|               Satan|   0116|      7|\n",
      "|                NULL|   3102|      7|\n",
      "|    Bias: Anti-Hindu|   1525|      7|\n",
      "|               Teach|   1024|      6|\n",
      "|                NULL|   2191|      6|\n",
      "|                NULL|   3602|      6|\n",
      "|                NULL|   2213|      6|\n",
      "| Bias: Anti-Buddhist|   1522|      6|\n",
      "|               Bless|   1006|      6|\n",
      "|                NULL|   2302|      6|\n",
      "|Bias: Anti-Multip...|   1530|      6|\n",
      "|                NULL|   2305|      6|\n",
      "|                NULL|   4014|      6|\n",
      "|                NULL|   4024|      6|\n",
      "|                NULL|   2199|      6|\n",
      "|  Was Transit Patrol|   0367|      6|\n",
      "|Bias: Anti-Americ...|   1511|      5|\n",
      "|                NULL|   2141|      5|\n",
      "|                NULL|   4013|      5|\n",
      "|                NULL|   3021|      5|\n",
      "|Non-Revocable Par...|   0941|      4|\n",
      "|       Subscriptions|   1023|      4|\n",
      "|                NULL|   2187|      4|\n",
      "|                NULL|   3020|      4|\n",
      "|            appraise|   1003|      4|\n",
      "|                NULL|   4020|      4|\n",
      "|                NULL|   4003|      4|\n",
      "|                NULL|   4006|      4|\n",
      "|                NULL|   2229|      4|\n",
      "|Suspect was Train...|   2016|      4|\n",
      "|                NULL|   2207|      4|\n",
      "|                NULL|   3040|      3|\n",
      "|                NULL|   2186|      3|\n",
      "|                NULL|   2112|      3|\n",
      "|                NULL|   2231|      3|\n",
      "|                NULL|   2142|      3|\n",
      "|Bias: Anti-Protes...|   1533|      3|\n",
      "|                NULL|   3002|      3|\n",
      "|Bias: Anti-Native...|   1518|      3|\n",
      "|                NULL|   2131|      3|\n",
      "|                NULL|   3012|      3|\n",
      "|                NULL|   2102|      3|\n",
      "|                NULL|   3103|      3|\n",
      "|                NULL|   3062|      3|\n",
      "|                NULL|   3005|      3|\n",
      "|Bias: Anti-Hetero...|   1537|      3|\n",
      "|                NULL|   4007|      3|\n",
      "|                NULL|   2194|      3|\n",
      "|                NULL|   2111|      2|\n",
      "|                NULL|   3038|      2|\n",
      "|                NULL|   2198|      2|\n",
      "|                NULL|   3019|      2|\n",
      "|                NULL|   3801|      2|\n",
      "|                NULL|   3501|      2|\n",
      "|                NULL|   3027|      2|\n",
      "|                NULL|   2233|      2|\n",
      "|                NULL|   2120|      2|\n",
      "|                NULL|   3064|      2|\n",
      "|                NULL|   2121|      2|\n",
      "|                NULL|   2159|      2|\n",
      "|                NULL|   2218|      2|\n",
      "|                NULL|   4001|      2|\n",
      "|                NULL|   2138|      2|\n",
      "|                NULL|   2146|      2|\n",
      "| Agricultural Target|   1263|      2|\n",
      "|                NULL|   2143|      2|\n",
      "|                NULL|   2171|      2|\n",
      "|                NULL|   2170|      1|\n",
      "|                NULL|   2312|      1|\n",
      "|                NULL|   2205|      1|\n",
      "|                NULL|   2124|      1|\n",
      "|                NULL|   2105|      1|\n",
      "|                NULL|   2181|      1|\n",
      "|                NULL|   3014|      1|\n",
      "|                NULL|   3063|      1|\n",
      "|                NULL|   3601|      1|\n",
      "|                NULL|   2310|      1|\n",
      "|Bias: Anti-Atheis...|   1521|      1|\n",
      "|                NULL|   2114|      1|\n",
      "|                NULL|   2306|      1|\n",
      "|                NULL|   2150|      1|\n",
      "|                NULL|   2211|      1|\n",
      "|                NULL|   2221|      1|\n",
      "|                NULL|   2139|      1|\n",
      "|     Bias: Anti-Sikh|   1534|      1|\n",
      "|Bias: Anti-Easter...|   1524|      1|\n",
      "|                NULL|   2222|      1|\n",
      "|                NULL|   2116|      1|\n",
      "|                NULL|   2192|      1|\n",
      "|                NULL|   2224|      1|\n",
      "|                NULL|   2173|      1|\n",
      "|                NULL|   2113|      1|\n",
      "|Suspect gives vic...|   0548|      1|\n",
      "|                NULL|   2309|      1|\n",
      "|                NULL|   3010|      1|\n",
      "|                NULL|   2162|      1|\n",
      "|                NULL|   2130|      1|\n",
      "|                NULL|   2123|      1|\n",
      "|                NULL|   2163|      1|\n",
      "|                NULL|   2164|      1|\n",
      "|                NULL|   4002|      1|\n",
      "|                NULL|   3023|      1|\n",
      "|                NULL|      -|      1|\n",
      "|                NULL|   3001|      1|\n",
      "+--------------------+-------+-------+\n",
      "\n",
      "\n",
      "Overall time: 4.42\n",
      "Number of rows of result table: 774\n",
      "Number of rows of mocodes.txt table: 615"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema definition (columns needed for this query)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load the two crime CSV files from S3\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Union the two DataFrames\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Split Mocodes string into an array of codes\n",
    "# ---------------------------------------------------------------------\n",
    "crime_mo = crime_df.withColumn(\n",
    "    \"MO_array\", F.split(F.col(\"Mocodes\"), \" \")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Explode into one row per MO code (drop empty codes)\n",
    "# ---------------------------------------------------------------------\n",
    "crime_mo = crime_mo.withColumn(\n",
    "    \"MO_code\", F.explode(\"MO_array\")\n",
    ").filter(F.col(\"MO_code\") != \"\")   # Πετάμε τα κενά strings\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load MO_codes dictionary (text file: e.g., \"0100 Suspect Impersonate\")\n",
    "# ---------------------------------------------------------------------\n",
    "mo_schema = StructType([\n",
    "    StructField(\"raw\", StringType())\n",
    "])\n",
    "\n",
    "mo_codes_raw = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "\n",
    "# Split each line into (code, description)\n",
    "mo_codes = mo_codes_raw.select(\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(0).alias(\"MO_code\"),\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(1).alias(\"MO_description\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Join crime MO codes with dictionary on MO_code (default join strategy)\n",
    "# LEFT join keeps all crime MO codes, even if they have no dictionary entry\n",
    "# ---------------------------------------------------------------------\n",
    "joined = crime_mo.join(mo_codes, on=\"MO_code\", how=\"left\")\n",
    "\n",
    "# Show physical plan (default strategy chosen by Catalyst)\n",
    "joined.explain(True)\n",
    "print(\"\\n====================================================================================================================================================================================\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Aggregate frequency per MO code / description and sort descending\n",
    "# ---------------------------------------------------------------------\n",
    "result = joined.groupBy(\"MO_code\",\"MO_description\") \\\n",
    "    .agg(F.count(\"*\").alias(\"freq\")) \\\n",
    "    .orderBy(F.col(\"freq\").desc()) \\\n",
    "    .select(\"MO_description\",\"MO_code\",\"freq\")\n",
    "\n",
    "end=time.time()\n",
    "overall_time=end-start\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Display full result and execution stats\n",
    "# ---------------------------------------------------------------------\n",
    "result.show(774)\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")\n",
    "\n",
    "# Number of rows in the aggregated result\n",
    "print(\"Number of rows of result table:\", result.count())\n",
    "\n",
    "# Number of rows in MO_codes dictionary\n",
    "print(\"Number of rows of mocodes.txt table:\", mo_codes.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd14a6-7d7e-4c52-9f49-95d7914b5844",
   "metadata": {},
   "source": [
    "## Observation – Missing MO codes between datasets\n",
    "\n",
    " Check which MO codes appear in the crime dataset but not in MO_codes.txt,\n",
    " and vice versa, to explain mismatched row counts and NULL descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "146b49df-3634-4298-a65b-81588cdbde01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e910709cff4924a8d429493c9fba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MO_codes of crime dataset that do not exist in MO_codes.txt:\n",
      "+-------+\n",
      "|MO_code|\n",
      "+-------+\n",
      "|   0851|\n",
      "|   0858|\n",
      "|   4021|\n",
      "|   4014|\n",
      "|   3026|\n",
      "|   2213|\n",
      "|   3032|\n",
      "|   2202|\n",
      "|   0859|\n",
      "|   4016|\n",
      "|   3029|\n",
      "|   2120|\n",
      "|   0855|\n",
      "|   4018|\n",
      "|   0857|\n",
      "|   2206|\n",
      "|   2118|\n",
      "|   0850|\n",
      "|   3039|\n",
      "|   2204|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "MO_codes of MO_codes.txt that do not exist in crime dataset:\n",
      "+-------+\n",
      "|MO_code|\n",
      "+-------+\n",
      "|persons|\n",
      "|   1529|\n",
      "+-------+"
     ]
    }
   ],
   "source": [
    "# Unique MO codes from the crime dataset (after explode)\n",
    "crime_mo_codes = crime_mo.select(\"MO_code\").distinct()\n",
    "\n",
    "# Unique MO codes from the MO_codes dictionary file\n",
    "mo_codes_unique = mo_codes.select(\"MO_code\").distinct()\n",
    "\n",
    "# MO codes that appear in the crime dataset but not in MO_codes.txt\n",
    "missing_in_txt = crime_mo_codes.join(mo_codes_unique, on=\"MO_code\", how=\"left_anti\")\n",
    "print(\"MO_codes of crime dataset that do not exist in MO_codes.txt:\")\n",
    "missing_in_txt.show()\n",
    "\n",
    "# MO codes that appear in MO_codes.txt but not in the crime dataset\n",
    "missing_in_crime = mo_codes_unique.join(crime_mo_codes, on=\"MO_code\", how=\"left_anti\")\n",
    "print(\"MO_codes of MO_codes.txt that do not exist in crime dataset:\")\n",
    "missing_in_crime.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952f307-7e04-4e08-8a2a-3e1ff82c5aef",
   "metadata": {},
   "source": [
    "# 3.2 DataFrame API – Broadcast hash join strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526a443e-122d-48d8-960f-6e4540ca18fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>120</td><td>application_1761923966900_0135</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0135/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-43.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0135_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc6d94d888a49c297a3c668d8e84e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590a2c7216194f60b93fa8150838a31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [MO_code])\n",
      ":- Filter NOT (MO_code#182 = )\n",
      ":  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      ":     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      ":        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      ":           +- Union false, false\n",
      ":              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      ":              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Relation [value#213] text\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "MO_code: string, DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, ... 7 more fields\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      "   :     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :           +- Union false, false\n",
      "   :              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215), rightHint=(strategy=broadcast)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :     +- Union false, false\n",
      "   :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "   :        :     +- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "   :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "   +- BroadcastHashJoin [MO_code#182], [MO_code#215], LeftOuter, BuildRight, false\n",
      "      :- Filter NOT (MO_code#182 = )\n",
      "      :  +- Generate explode(MO_array#151), [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], false, [MO_code#182]\n",
      "      :     +- Union\n",
      "      :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "      :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "      :        :     +- FileScan csv [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#10,  , -1), true) > 0), isnotnull(split(Mocodes#10,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "      :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "      :              +- FileScan csv [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#77,  , -1), true) > 0), isnotnull(split(Mocodes#77,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=43]\n",
      "         +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "            +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "               +- FileScan text [value#213] Batched: false, DataFilters: [NOT (split(value#213,  , 2)[0] = ), isnotnull(split(value#213,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================\n",
      "\n",
      "+--------------------+-------+-------+\n",
      "|      MO_description|MO_code|   freq|\n",
      "+--------------------+-------+-------+\n",
      "|Removes vict prop...|   0344|1002900|\n",
      "|            Stranger|   1822| 548422|\n",
      "|   Hit-Hit w/ weapon|   0416| 404773|\n",
      "|          Vandalized|   0329| 377536|\n",
      "| Victim knew Suspect|   0913| 278618|\n",
      "|   Domestic violence|   2000| 256188|\n",
      "|    Vehicle involved|   1300| 219082|\n",
      "|          Force used|   0400| 213165|\n",
      "|Evidence Booked (...|   1402| 177470|\n",
      "|             Smashed|   1609| 131229|\n",
      "|   Susp uses vehicle|   1309| 122108|\n",
      "|Victim was aged (...|   1202| 120238|\n",
      "|    Took merchandise|   0325| 120159|\n",
      "|Susp is/was curre...|   1814| 118073|\n",
      "|              Pushed|   0444| 116763|\n",
      "|  Other MO (see rpt)|   1501| 115589|\n",
      "|       Breaks window|   1307| 113609|\n",
      "|   Brandishes weapon|   0334| 105665|\n",
      "|Suspect is homele...|   2004|  93426|\n",
      "|        Intimidation|   0432|  83562|\n",
      "+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Overall time: 4.20"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema definition\n",
    "# ---------------------------------------------------------------------\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load and union crime datasets\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Split and explode Mocodes\n",
    "crime_mo = crime_df.withColumn(\n",
    "    \"MO_array\", F.split(F.col(\"Mocodes\"), \" \")\n",
    ")\n",
    "\n",
    "crime_mo = crime_mo.withColumn(\n",
    "    \"MO_code\", F.explode(\"MO_array\")\n",
    ").filter(F.col(\"MO_code\") != \"\")   # Πετάμε τα κενά strings\n",
    "\n",
    "# Load MO_codes dictionary\n",
    "mo_schema = StructType([\n",
    "    StructField(\"raw\", StringType())\n",
    "])\n",
    "\n",
    "mo_codes_raw = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "\n",
    "# Brake the line in 2 parts: code + description\n",
    "mo_codes = mo_codes_raw.select(\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(0).alias(\"MO_code\"),\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(1).alias(\"MO_description\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Broadcast hash join hint on the MO_codes side\n",
    "# ---------------------------------------------------------------------\n",
    "joined = crime_mo.join(\n",
    "    mo_codes.hint(\"broadcast\"),\n",
    "    on=\"MO_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "joined.explain(True)\n",
    "print(\"\\n====================================================================================================================================================================================\\n\")\n",
    "\n",
    "# Aggregate frequency and display\n",
    "result = joined.groupBy(\"MO_code\",\"MO_description\") \\\n",
    "    .agg(F.count(\"*\").alias(\"freq\")) \\\n",
    "    .orderBy(F.col(\"freq\").desc()) \\\n",
    "    .select(\"MO_description\",\"MO_code\",\"freq\")\n",
    "\n",
    "end=time.time()\n",
    "overall_time=end-start\n",
    "\n",
    "result.show(20)\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2609d-b2af-47a2-9a75-367e77f3ad8c",
   "metadata": {},
   "source": [
    "## DataFrame API – Sort-merge join strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2ac0ac-87da-4304-ad15-270e0521fe72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>121</td><td>application_1761923966900_0136</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0136/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-92.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0136_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c811f45db3464b80ace661be11efd335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c63fb97e3b42569789ae1b6a9dd018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [MO_code])\n",
      ":- Filter NOT (MO_code#182 = )\n",
      ":  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      ":     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      ":        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      ":           +- Union false, false\n",
      ":              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      ":              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=merge)\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Relation [value#213] text\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "MO_code: string, DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, ... 7 more fields\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      "   :     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :           +- Union false, false\n",
      "   :              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- ResolvedHint (strategy=merge)\n",
      "      +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215), rightHint=(strategy=merge)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :     +- Union false, false\n",
      "   :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "   :        :     +- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "   :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "   +- SortMergeJoin [MO_code#182], [MO_code#215], LeftOuter\n",
      "      :- Sort [MO_code#182 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(MO_code#182, 1000), ENSURE_REQUIREMENTS, [plan_id=44]\n",
      "      :     +- Filter NOT (MO_code#182 = )\n",
      "      :        +- Generate explode(MO_array#151), [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], false, [MO_code#182]\n",
      "      :           +- Union\n",
      "      :              :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "      :              :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "      :              :     +- FileScan csv [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#10,  , -1), true) > 0), isnotnull(split(Mocodes#10,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      :              +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "      :                 +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "      :                    +- FileScan csv [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#77,  , -1), true) > 0), isnotnull(split(Mocodes#77,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      +- Sort [MO_code#215 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(MO_code#215, 1000), ENSURE_REQUIREMENTS, [plan_id=45]\n",
      "            +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "               +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "                  +- FileScan text [value#213] Batched: false, DataFilters: [NOT (split(value#213,  , 2)[0] = ), isnotnull(split(value#213,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================\n",
      "\n",
      "+--------------------+-------+-------+\n",
      "|      MO_description|MO_code|   freq|\n",
      "+--------------------+-------+-------+\n",
      "|Removes vict prop...|   0344|1002900|\n",
      "|            Stranger|   1822| 548422|\n",
      "|   Hit-Hit w/ weapon|   0416| 404773|\n",
      "|          Vandalized|   0329| 377536|\n",
      "| Victim knew Suspect|   0913| 278618|\n",
      "|   Domestic violence|   2000| 256188|\n",
      "|    Vehicle involved|   1300| 219082|\n",
      "|          Force used|   0400| 213165|\n",
      "|Evidence Booked (...|   1402| 177470|\n",
      "|             Smashed|   1609| 131229|\n",
      "|   Susp uses vehicle|   1309| 122108|\n",
      "|Victim was aged (...|   1202| 120238|\n",
      "|    Took merchandise|   0325| 120159|\n",
      "|Susp is/was curre...|   1814| 118073|\n",
      "|              Pushed|   0444| 116763|\n",
      "|  Other MO (see rpt)|   1501| 115589|\n",
      "|       Breaks window|   1307| 113609|\n",
      "|   Brandishes weapon|   0334| 105665|\n",
      "|Suspect is homele...|   2004|  93426|\n",
      "|        Intimidation|   0432|  83562|\n",
      "+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Overall time: 4.44"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema definition\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load and union crime datasets\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Split and explode Mocodes\n",
    "crime_mo = crime_df.withColumn(\n",
    "    \"MO_array\", F.split(F.col(\"Mocodes\"), \" \")\n",
    ")\n",
    "\n",
    "crime_mo = crime_mo.withColumn(\n",
    "    \"MO_code\", F.explode(\"MO_array\")\n",
    ").filter(F.col(\"MO_code\") != \"\")   # Πετάμε τα κενά strings\n",
    "\n",
    "# Load MO_codes dictionary\n",
    "mo_schema = StructType([\n",
    "    StructField(\"raw\", StringType())\n",
    "])\n",
    "\n",
    "mo_codes_raw = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "\n",
    "# BRake the line in 2 parts: code + description\n",
    "mo_codes = mo_codes_raw.select(\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(0).alias(\"MO_code\"),\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(1).alias(\"MO_description\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Sort-merge join hint\n",
    "# ---------------------------------------------------------------------\n",
    "joined = crime_mo.join(\n",
    "    mo_codes.hint(\"merge\"),\n",
    "    on=\"MO_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "joined.explain(True)\n",
    "print(\"\\n====================================================================================================================================================================================\\n\")\n",
    "\n",
    "# ----- 6. Count συχνότητα εμφάνισης -----\n",
    "result = joined.groupBy(\"MO_code\",\"MO_description\") \\\n",
    "    .agg(F.count(\"*\").alias(\"freq\")) \\\n",
    "    .orderBy(F.col(\"freq\").desc()) \\\n",
    "    .select(\"MO_description\",\"MO_code\",\"freq\")\n",
    "\n",
    "end=time.time()\n",
    "overall_time=end-start\n",
    "\n",
    "result.show(20)\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a925cd5-fb78-4f8f-ad96-4c3c6a7b43b3",
   "metadata": {},
   "source": [
    "## DataFrame API – Shuffle hash join strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0214ffb-d3cf-4f31-8e70-37e275dade8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>391</td><td>application_1765289937462_0387</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0387/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0387_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9077801e71447fa6196c30395d40a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a628293c314fea9db8a784cdece090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [MO_code])\n",
      ":- Filter NOT (MO_code#182 = )\n",
      ":  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      ":     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      ":        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      ":           +- Union false, false\n",
      ":              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      ":              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_hash)\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Relation [value#213] text\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "MO_code: string, DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, ... 7 more fields\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      "   :     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :           +- Union false, false\n",
      "   :              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- ResolvedHint (strategy=shuffle_hash)\n",
      "      +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215), rightHint=(strategy=shuffle_hash)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :     +- Union false, false\n",
      "   :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "   :        :     +- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "   :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "   +- ShuffledHashJoin [MO_code#182], [MO_code#215], LeftOuter, BuildRight\n",
      "      :- Exchange hashpartitioning(MO_code#182, 1000), ENSURE_REQUIREMENTS, [plan_id=44]\n",
      "      :  +- Filter NOT (MO_code#182 = )\n",
      "      :     +- Generate explode(MO_array#151), [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], false, [MO_code#182]\n",
      "      :        +- Union\n",
      "      :           :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "      :           :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "      :           :     +- FileScan csv [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#10,  , -1), true) > 0), isnotnull(split(Mocodes#10,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      :           +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "      :              +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "      :                 +- FileScan csv [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#77,  , -1), true) > 0), isnotnull(split(Mocodes#77,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      +- Exchange hashpartitioning(MO_code#215, 1000), ENSURE_REQUIREMENTS, [plan_id=45]\n",
      "         +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "            +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "               +- FileScan text [value#213] Batched: false, DataFilters: [NOT (split(value#213,  , 2)[0] = ), isnotnull(split(value#213,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================\n",
      "\n",
      "+--------------------+-------+-------+\n",
      "|      MO_description|MO_code|   freq|\n",
      "+--------------------+-------+-------+\n",
      "|Removes vict prop...|   0344|1002900|\n",
      "|            Stranger|   1822| 548422|\n",
      "|   Hit-Hit w/ weapon|   0416| 404773|\n",
      "|          Vandalized|   0329| 377536|\n",
      "| Victim knew Suspect|   0913| 278618|\n",
      "|   Domestic violence|   2000| 256188|\n",
      "|    Vehicle involved|   1300| 219082|\n",
      "|          Force used|   0400| 213165|\n",
      "|Evidence Booked (...|   1402| 177470|\n",
      "|             Smashed|   1609| 131229|\n",
      "|   Susp uses vehicle|   1309| 122108|\n",
      "|Victim was aged (...|   1202| 120238|\n",
      "|    Took merchandise|   0325| 120159|\n",
      "|Susp is/was curre...|   1814| 118073|\n",
      "|              Pushed|   0444| 116763|\n",
      "|  Other MO (see rpt)|   1501| 115589|\n",
      "|       Breaks window|   1307| 113609|\n",
      "|   Brandishes weapon|   0334| 105665|\n",
      "|Suspect is homele...|   2004|  93426|\n",
      "|        Intimidation|   0432|  83562|\n",
      "+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Overall time: 7.59"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema definition\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load and union crime datasets\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Split and explode Mocodes\n",
    "crime_mo = crime_df.withColumn(\n",
    "    \"MO_array\", F.split(F.col(\"Mocodes\"), \" \")\n",
    ")\n",
    "\n",
    "crime_mo = crime_mo.withColumn(\n",
    "    \"MO_code\", F.explode(\"MO_array\")\n",
    ").filter(F.col(\"MO_code\") != \"\")   # Πετάμε τα κενά strings\n",
    "\n",
    "# Load MO_codes dictionary\n",
    "mo_schema = StructType([\n",
    "    StructField(\"raw\", StringType())\n",
    "])\n",
    "\n",
    "mo_codes_raw = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "\n",
    "# Brake the line 2 parts: code + description\n",
    "mo_codes = mo_codes_raw.select(\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(0).alias(\"MO_code\"),\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(1).alias(\"MO_description\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Shuffle hash join hint\n",
    "# ---------------------------------------------------------------------\n",
    "joined = crime_mo.join(\n",
    "    mo_codes.hint(\"shuffle_hash\"),\n",
    "    on=\"MO_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "joined.explain(True)\n",
    "print(\"\\n====================================================================================================================================================================================\\n\")\n",
    "\n",
    "# ----- 6. Count frequency -----\n",
    "result = joined.groupBy(\"MO_code\",\"MO_description\") \\\n",
    "    .agg(F.count(\"*\").alias(\"freq\")) \\\n",
    "    .orderBy(F.col(\"freq\").desc()) \\\n",
    "    .select(\"MO_description\",\"MO_code\",\"freq\")\n",
    "\n",
    "end=time.time()\n",
    "overall_time=end-start\n",
    "\n",
    "result.show(20)\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8e9c6-d1f8-4b07-83d4-bf800a4d0398",
   "metadata": {},
   "source": [
    "## DataFrame API – Shuffle replicate nested loop join strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f666ab-f57c-47ee-8e74-c7b13f5066f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>122</td><td>application_1761923966900_0137</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0137/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-97.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0137_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73030185969f4499bb13923b732494b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38656049bf04dfcb07c123b63731951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [MO_code])\n",
      ":- Filter NOT (MO_code#182 = )\n",
      ":  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      ":     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      ":        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      ":           +- Union false, false\n",
      ":              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      ":              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Relation [value#213] text\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "MO_code: string, DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, ... 7 more fields\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 6 more fields]\n",
      "   :     +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :        +- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :           +- Union false, false\n",
      "   :              :- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "      +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "+- Join LeftOuter, (MO_code#182 = MO_code#215), rightHint=(strategy=shuffle_replicate_nl)\n",
      "   :- Filter NOT (MO_code#182 = )\n",
      "   :  +- Generate explode(MO_array#151), false, [MO_code#182]\n",
      "   :     +- Union false, false\n",
      "   :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "   :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "   :        :     +- Relation [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] csv\n",
      "   :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "   :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "   :              +- Relation [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] csv\n",
      "   +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "      +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "         +- Relation [value#213] text\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [MO_code#182, DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, ... 7 more fields]\n",
      "   +- BroadcastHashJoin [MO_code#182], [MO_code#215], LeftOuter, BuildRight, false\n",
      "      :- Filter NOT (MO_code#182 = )\n",
      "      :  +- Generate explode(MO_array#151), [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], false, [MO_code#182]\n",
      "      :     +- Union\n",
      "      :        :- Project [DR_NO#0, Date Rptd#1, DATE OCC#2, TIME OCC#3, AREA#4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "      :        :  +- Filter ((size(split(Mocodes#10,  , -1), true) > 0) AND isnotnull(split(Mocodes#10,  , -1)))\n",
      "      :        :     +- FileScan csv [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA#4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#10,  , -1), true) > 0), isnotnull(split(Mocodes#10,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      :        +- Project [DR_NO#67, Date Rptd#68, DATE OCC#69, TIME OCC#70, AREA#71, AREA NAME#72, Rpt Dist No#73, Part 1-2#74, Crm Cd#75, Crm Cd Desc#76, Mocodes#77, Vict Age#78, Vict Sex#79, Vict Descent#80, Premis Cd#81, Premis Desc#82, Weapon Used Cd#83, Weapon Desc#84, Status#85, Status Desc#86, Crm Cd 1#87, Crm Cd 2#88, Crm Cd 3#89, Crm Cd 4#90, ... 5 more fields]\n",
      "      :           +- Filter ((size(split(Mocodes#77,  , -1), true) > 0) AND isnotnull(split(Mocodes#77,  , -1)))\n",
      "      :              +- FileScan csv [DR_NO#67,Date Rptd#68,DATE OCC#69,TIME OCC#70,AREA#71,AREA NAME#72,Rpt Dist No#73,Part 1-2#74,Crm Cd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,Vict Sex#79,Vict Descent#80,Premis Cd#81,Premis Desc#82,Weapon Used Cd#83,Weapon Desc#84,Status#85,Status Desc#86,Crm Cd 1#87,Crm Cd 2#88,Crm Cd 3#89,Crm Cd 4#90,... 4 more fields] Batched: false, DataFilters: [(size(split(Mocodes#77,  , -1), true) > 0), isnotnull(split(Mocodes#77,  , -1))], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=43]\n",
      "         +- Project [split(value#213,  , 2)[0] AS MO_code#215, split(value#213,  , 2)[1] AS MO_description#216]\n",
      "            +- Filter (NOT (split(value#213,  , 2)[0] = ) AND isnotnull(split(value#213,  , 2)[0]))\n",
      "               +- FileScan text [value#213] Batched: false, DataFilters: [NOT (split(value#213,  , 2)[0] = ), isnotnull(split(value#213,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================\n",
      "\n",
      "+--------------------+-------+-------+\n",
      "|      MO_description|MO_code|   freq|\n",
      "+--------------------+-------+-------+\n",
      "|Removes vict prop...|   0344|1002900|\n",
      "|            Stranger|   1822| 548422|\n",
      "|   Hit-Hit w/ weapon|   0416| 404773|\n",
      "|          Vandalized|   0329| 377536|\n",
      "| Victim knew Suspect|   0913| 278618|\n",
      "|   Domestic violence|   2000| 256188|\n",
      "|    Vehicle involved|   1300| 219082|\n",
      "|          Force used|   0400| 213165|\n",
      "|Evidence Booked (...|   1402| 177470|\n",
      "|             Smashed|   1609| 131229|\n",
      "|   Susp uses vehicle|   1309| 122108|\n",
      "|Victim was aged (...|   1202| 120238|\n",
      "|    Took merchandise|   0325| 120159|\n",
      "|Susp is/was curre...|   1814| 118073|\n",
      "|              Pushed|   0444| 116763|\n",
      "|  Other MO (see rpt)|   1501| 115589|\n",
      "|       Breaks window|   1307| 113609|\n",
      "|   Brandishes weapon|   0334| 105665|\n",
      "|Suspect is homele...|   2004|  93426|\n",
      "|        Intimidation|   0432|  83562|\n",
      "+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Overall time: 3.89"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema definition\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load and union crime datasets\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "\n",
    "# Split and explode Mocodes\n",
    "crime_mo = crime_df.withColumn(\n",
    "    \"MO_array\", F.split(F.col(\"Mocodes\"), \" \")\n",
    ")\n",
    "\n",
    "crime_mo = crime_mo.withColumn(\n",
    "    \"MO_code\", F.explode(\"MO_array\")\n",
    ").filter(F.col(\"MO_code\") != \"\")   # Πετάμε τα κενά strings\n",
    "\n",
    "# Load MO_codes dictionary\n",
    "mo_schema = StructType([\n",
    "    StructField(\"raw\", StringType())\n",
    "])\n",
    "\n",
    "mo_codes_raw = spark.read.text(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "\n",
    "# Brake the line in 2 parts: code + description\n",
    "mo_codes = mo_codes_raw.select(\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(0).alias(\"MO_code\"),\n",
    "    F.split(F.col(\"value\"), \" \", 2).getItem(1).alias(\"MO_description\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Shuffle replicate nested-loop join hint\n",
    "# ---------------------------------------------------------------------\n",
    "joined = crime_mo.join(\n",
    "    mo_codes.hint(\"shuffle_replicate_nl\"),\n",
    "    on=\"MO_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "joined.explain(True)\n",
    "print(\"\\n====================================================================================================================================================================================\\n\")\n",
    "\n",
    "# ----- 6. Count frequency -----\n",
    "result = joined.groupBy(\"MO_code\",\"MO_description\") \\\n",
    "    .agg(F.count(\"*\").alias(\"freq\")) \\\n",
    "    .orderBy(F.col(\"freq\").desc()) \\\n",
    "    .select(\"MO_description\",\"MO_code\",\"freq\")\n",
    "\n",
    "end=time.time()\n",
    "overall_time=end-start\n",
    "\n",
    "result.show(20)\n",
    "print(f\"\\nOverall time: {overall_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb52110-ea7f-4287-86ff-c158b9d05756",
   "metadata": {},
   "source": [
    "## RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ad0155-37a3-4e61-8bc5-022b5597ca95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>299</td><td>application_1761923966900_0314</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0314/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-133.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0314_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229bc890e278443187873f4ea02ec2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c582025e4c5e46e29c9195ec4f08ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0344', 1002900, 'Removes vict property')\n",
      "('1822', 548422, 'Stranger')\n",
      "('0416', 404773, 'Hit-Hit w/ weapon')\n",
      "('0329', 377536, 'Vandalized')\n",
      "('0913', 278618, 'Victim knew Suspect')\n",
      "('2000', 256188, 'Domestic violence')\n",
      "('1300', 219082, 'Vehicle involved')\n",
      "('0400', 213165, 'Force used')\n",
      "('1402', 177470, 'Evidence Booked (any crime)')\n",
      "('1609', 131229, 'Smashed')\n",
      "('1309', 122108, 'Susp uses vehicle')\n",
      "('1202', 120238, 'Victim was aged (60 & over) or blind/physically disabled/unable to care for self')\n",
      "('0325', 120159, 'Took merchandise')\n",
      "('1814', 118073, 'Susp is/was current/former boyfriend/girlfriend')\n",
      "('0444', 116763, 'Pushed')\n",
      "('1501', 115589, 'Other MO (see rpt)')\n",
      "('1307', 113609, 'Breaks window')\n",
      "('0334', 105665, 'Brandishes weapon')\n",
      "('2004', 93426, 'Suspect is homeless/transient')\n",
      "('0432', 83562, 'Intimidation')\n",
      "\n",
      "Overall time: 17.53 sec"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query3 RDD API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper: safe CSV parsing using Python's csv.reader\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_csv(line):\n",
    "    # Handles commas inside quotes and other CSV edge cases\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Read both crime CSV files as RDDs of text lines\n",
    "# ---------------------------------------------------------------------\n",
    "crime_rdd1 = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    ")\n",
    "crime_rdd2 = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Remove headers from both RDDs\n",
    "# ---------------------------------------------------------------------\n",
    "header1 = crime_rdd1.first()\n",
    "header2 = crime_rdd2.first()\n",
    "crime_rdd1 = crime_rdd1.filter(lambda x: x != header1)\n",
    "crime_rdd2 = crime_rdd2.filter(lambda x: x != header2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Union and parse CSV lines\n",
    "# ---------------------------------------------------------------------\n",
    "crime_rdd = crime_rdd1.union(crime_rdd2)\n",
    "crime_rdd = crime_rdd.map(parse_csv)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Extract MO codes from Mocodes column (index 10)\n",
    "# Split on whitespace and emit (code, 1) for each non-empty code\n",
    "# ---------------------------------------------------------------------\n",
    "mocodes_rdd = crime_rdd.flatMap(lambda row: [(code, 1) for code in row[10].split() if code.strip() != \"\"])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Reduce by key to get frequency per MO code\n",
    "# ---------------------------------------------------------------------\n",
    "mo_counts_rdd = mocodes_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load MO_codes.txt as (code, description)\n",
    "# ---------------------------------------------------------------------\n",
    "mo_rdd = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    ")\n",
    "mo_rdd = mo_rdd.map(lambda line: line.strip().split(\" \", 1))  # (code, description)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Left outer join: keep all MO codes from crime data\n",
    "# ---------------------------------------------------------------------\n",
    "joined_rdd = mo_counts_rdd.leftOuterJoin(mo_rdd) \\\n",
    "                          .map(lambda x: (x[0], x[1][0], x[1][1]))  # (mocode, count, description)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Sort by frequency (descending) and display top 20 codes\n",
    "# ---------------------------------------------------------------------\n",
    "final_rdd = joined_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "for row in final_rdd.take(20):\n",
    "    print(row)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nOverall time: {end - start:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd47e4-6c83-417d-ac72-94936be946a0",
   "metadata": {},
   "source": [
    "# Query 4 – Nearest police station and average distance per division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689a802-3678-430b-b68a-206e62836bc2",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "Using Apache Sedona and the LA crime + police station datasets:\n",
    "\n",
    "1. Represent each crime and police station as a geospatial POINT.\n",
    "\n",
    "2. For every crime, find the nearest police station based on spherical distance.\n",
    "\n",
    "3.Aggregate, per police division, the:\n",
    "\n",
    " - Average distance from crimes to their closest station (in meters)\n",
    "\n",
    " - Total number of crimes assigned to that division\n",
    "\n",
    "4. Compare performance under different cluster configurations:\n",
    "\n",
    " - 1 core, 2 GB per executor\n",
    "\n",
    " - 2 cores, 4 GB per executor\n",
    "\n",
    " - 4 cores, 8 GB per executor\n",
    "\n",
    "All three implementations use the same DataFrame logic; only the Spark resource configuration changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93425dc-09dd-4339-86a8-fa71ad2cc2ce",
   "metadata": {},
   "source": [
    "## 4.1 DataFrame + Sedona – 1 core, 2 GB per executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48562019-2f13-4fcf-b3c1-572f4fc5b610",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>304</td><td>application_1761923966900_0319</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0319/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-65.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0319_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b523a1b9f0d84e7499be05f8e556252f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a94d5b7ae504727bd20fc8f2b5e2e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, Crm Cd 4: string, ... 12 more fields\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Cross\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  :  +- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "   :- Union\n",
      "   :  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      "   :  :  +- FileScan csv [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   :  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      "   :     +- FileScan csv [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=53]\n",
      "      +- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "         +- FileScan csv [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:double,Y:double,FID:int,DIVISION:string,LOCATION:string,PREC:int>\n",
      "\n",
      "\n",
      "Overall time: 2.23 sec\n",
      "+----------------+--------------+-----------+\n",
      "|        DIVISION|avg_distance_m|crime_count|\n",
      "+----------------+--------------+-----------+\n",
      "|       HOLLYWOOD|       2073.58|     224124|\n",
      "|        VAN NUYS|       2939.26|     208129|\n",
      "|       SOUTHWEST|       2191.47|     189119|\n",
      "|        WILSHIRE|        2593.3|     186383|\n",
      "|     77TH STREET|       1717.11|     170620|\n",
      "| NORTH HOLLYWOOD|        2642.5|     168096|\n",
      "|         OLYMPIC|       1728.93|     162805|\n",
      "|         PACIFIC|        3853.5|     162027|\n",
      "|         CENTRAL|        993.32|     154689|\n",
      "|         RAMPART|       1534.22|     153204|\n",
      "|       SOUTHEAST|       2443.91|     143803|\n",
      "|     WEST VALLEY|       3021.57|     136622|\n",
      "|        FOOTHILL|        4260.1|     132482|\n",
      "|         TOPANGA|       3296.99|     131054|\n",
      "|          HARBOR|       3701.72|     127071|\n",
      "|      HOLLENBECK|     333909.08|     119381|\n",
      "|WEST LOS ANGELES|       2789.52|     115969|\n",
      "|          NEWTON|       1635.23|     111392|\n",
      "|       NORTHEAST|       3622.99|     108243|\n",
      "|         MISSION|       3676.23|      97926|\n",
      "+----------------+--------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import  round, col, broadcast, row_number, avg, count \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration (1 core, 2 GB per executor)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "# Register Sedona functions (ST_Point, ST_DistanceSphere, etc.)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Schema definitions for crime data and police station data\n",
    "# ---------------------------------------------------------------------\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "police_schema = StructType([\n",
    "    StructField(\"X\", DoubleType()),          \n",
    "    StructField(\"Y\", DoubleType()),          \n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load crime data from both CSV files and union\n",
    "# ---------------------------------------------------------------------\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crimes_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load police station dataset\n",
    "# ---------------------------------------------------------------------\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\", header=True, schema=police_schema)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Create geospatial POINT objects for crimes and stations\n",
    "# ---------------------------------------------------------------------\n",
    "crimes_df = crimes_df \\\n",
    "    .withColumn(\"crime_point\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "stations_df = stations_df \\\n",
    "    .withColumn(\"station_point\", ST_Point(col(\"X\"), col(\"Y\")))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Cross join crimes ↔ stations\n",
    "# (Cartesian product to later pick the nearest station per crime)\n",
    "# ---------------------------------------------------------------------\n",
    "joined_df = crimes_df.crossJoin(stations_df)\n",
    "\n",
    "joined_df.explain(True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Compute spherical distance (in meters) between crime and station\n",
    "# ---------------------------------------------------------------------\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"distance_m\",\n",
    "    ST_DistanceSphere(col(\"crime_point\"), col(\"station_point\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# For each crime, keep only the nearest station (min distance)\n",
    "# ---------------------------------------------------------------------\n",
    "window = Window.partitionBy(\"DR_NO\").orderBy(asc(\"distance_m\"))\n",
    "\n",
    "closest_df = joined_df.withColumn(\n",
    "    \"rn\",\n",
    "    row_number().over(window)\n",
    ").filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Aggregate per division: average distance + crime count\n",
    "# ---------------------------------------------------------------------\n",
    "result_df = closest_df.groupBy(\"DIVISION\").agg(\n",
    "    round(avg(\"distance_m\"),2).alias(\"avg_distance_m\"),\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ").orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nOverall time: {end - start:.2f} sec\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Display result (reordered columns)\n",
    "# ---------------------------------------------------------------------\n",
    "# reorder columns\n",
    "result_df = result_df.select(\n",
    "    \"DIVISION\",\n",
    "    \"avg_distance_m\",\n",
    "    \"crime_count\"   # εδώ το βάζεις τελευταίο\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1e46c-319c-4dfc-85aa-fe3e22490c04",
   "metadata": {},
   "source": [
    "## 4.2 DataFrame + Sedona – 2 cores, 4 GB per executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15def372-5df4-4997-be3a-d92ecbed12a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>303</td><td>application_1761923966900_0318</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0318/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-175.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0318_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885867ddcd354e058ac756beabd80f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dc0ae8a788423381421d8a141db271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, Crm Cd 4: string, ... 12 more fields\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Cross\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  :  +- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "   :- Union\n",
      "   :  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      "   :  :  +- FileScan csv [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   :  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      "   :     +- FileScan csv [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=53]\n",
      "      +- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "         +- FileScan csv [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:double,Y:double,FID:int,DIVISION:string,LOCATION:string,PREC:int>\n",
      "\n",
      "\n",
      "Overall time: 2.12 sec\n",
      "+----------------+--------------+-----------+\n",
      "|        DIVISION|avg_distance_m|crime_count|\n",
      "+----------------+--------------+-----------+\n",
      "|       HOLLYWOOD|       2073.58|     224124|\n",
      "|        VAN NUYS|       2939.26|     208129|\n",
      "|       SOUTHWEST|       2191.47|     189119|\n",
      "|        WILSHIRE|        2593.3|     186383|\n",
      "|     77TH STREET|       1717.11|     170620|\n",
      "| NORTH HOLLYWOOD|        2642.5|     168096|\n",
      "|         OLYMPIC|       1728.93|     162805|\n",
      "|         PACIFIC|        3853.5|     162027|\n",
      "|         CENTRAL|        993.32|     154689|\n",
      "|         RAMPART|       1534.22|     153204|\n",
      "|       SOUTHEAST|       2443.91|     143803|\n",
      "|     WEST VALLEY|       3021.57|     136622|\n",
      "|        FOOTHILL|        4260.1|     132482|\n",
      "|         TOPANGA|       3296.99|     131054|\n",
      "|          HARBOR|       3701.72|     127071|\n",
      "|      HOLLENBECK|     333909.08|     119381|\n",
      "|WEST LOS ANGELES|       2789.52|     115969|\n",
      "|          NEWTON|       1635.23|     111392|\n",
      "|       NORTHEAST|       3622.99|     108243|\n",
      "|         MISSION|       3676.23|      97926|\n",
      "+----------------+--------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import round,col, broadcast, row_number, avg, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration (2 cores, 4 GB per executor)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Schemascrime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "police_schema = StructType([\n",
    "    StructField(\"X\", DoubleType()),          \n",
    "    StructField(\"Y\", DoubleType()),          \n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load crime data and union\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crimes_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Load stations data\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\", header=True, schema=police_schema)\n",
    "\n",
    "\n",
    "# Create POINT geometries\n",
    "crimes_df = crimes_df \\\n",
    "    .withColumn(\"crime_point\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "stations_df = stations_df \\\n",
    "    .withColumn(\"station_point\", ST_Point(col(\"X\"), col(\"Y\")))\n",
    "\n",
    "# Cross join crimes ↔ stations\n",
    "joined_df = crimes_df.crossJoin(stations_df)\n",
    "\n",
    "joined_df.explain(True)\n",
    "\n",
    "# Distance computation\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"distance_m\",\n",
    "    ST_DistanceSphere(col(\"crime_point\"), col(\"station_point\"))\n",
    ")\n",
    "\n",
    "# Nearest station per crime\n",
    "window = Window.partitionBy(\"DR_NO\").orderBy(asc(\"distance_m\"))\n",
    "\n",
    "closest_df = joined_df.withColumn(\n",
    "    \"rn\",\n",
    "    row_number().over(window)\n",
    ").filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "\n",
    "# Aggregation per division\n",
    "result_df = closest_df.groupBy(\"DIVISION\").agg(\n",
    "    round(avg(\"distance_m\"),2).alias(\"avg_distance_m\"),\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ").orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nOverall time: {end - start:.2f} sec\")\n",
    "\n",
    "# Reorder and show\n",
    "result_df = result_df.select(\n",
    "    \"DIVISION\",\n",
    "    \"avg_distance_m\",\n",
    "    \"crime_count\"   # εδώ το βάζεις τελευταίο\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e3c3b-c5b5-4992-84cb-132d842e5701",
   "metadata": {},
   "source": [
    "## 4.3 DataFrame + Sedona – 4 cores, 8 GB per executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13393ce1-acc8-4f60-8be1-4f0520ba7198",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>305</td><td>application_1761923966900_0320</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0320/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0320_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93e07d19c7e4535a9d5068c8ec3ba00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1031f2dab82942d9b95cd2d65a0aa402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: string, Date Rptd: string, DATE OCC: string, TIME OCC: string, AREA: string, AREA NAME: string, Rpt Dist No: string, Part 1-2: string, Crm Cd: string, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: string, Premis Desc: string, Weapon Used Cd: string, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: string, Crm Cd 2: string, Crm Cd 3: string, Crm Cd 4: string, ... 12 more fields\n",
      "Join Cross\n",
      ":- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  +- Union false, false\n",
      ":     :- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Cross\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      ":  :  +- Relation [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      ":     +- Relation [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] csv\n",
      "+- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "   +- Relation [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "   :- Union\n",
      "   :  :- Project [DR_NO#24, Date Rptd#25, DATE OCC#26, TIME OCC#27, AREA#28, AREA NAME#29, Rpt Dist No#30, Part 1-2#31, Crm Cd#32, Crm Cd Desc#33, Mocodes#34, Vict Age#35, Vict Sex#36, Vict Descent#37, Premis Cd#38, Premis Desc#39, Weapon Used Cd#40, Weapon Desc#41, Status#42, Status Desc#43, Crm Cd 1#44, Crm Cd 2#45, Crm Cd 3#46, Crm Cd 4#47, ... 5 more fields]\n",
      "   :  :  +- FileScan csv [DR_NO#24,Date Rptd#25,DATE OCC#26,TIME OCC#27,AREA#28,AREA NAME#29,Rpt Dist No#30,Part 1-2#31,Crm Cd#32,Crm Cd Desc#33,Mocodes#34,Vict Age#35,Vict Sex#36,Vict Descent#37,Premis Cd#38,Premis Desc#39,Weapon Used Cd#40,Weapon Desc#41,Status#42,Status Desc#43,Crm Cd 1#44,Crm Cd 2#45,Crm Cd 3#46,Crm Cd 4#47,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   :  +- Project [DR_NO#80, Date Rptd#81, DATE OCC#82, TIME OCC#83, AREA#84, AREA NAME#85, Rpt Dist No#86, Part 1-2#87, Crm Cd#88, Crm Cd Desc#89, Mocodes#90, Vict Age#91, Vict Sex#92, Vict Descent#93, Premis Cd#94, Premis Desc#95, Weapon Used Cd#96, Weapon Desc#97, Status#98, Status Desc#99, Crm Cd 1#100, Crm Cd 2#101, Crm Cd 3#102, Crm Cd 4#103, ... 5 more fields]\n",
      "   :     +- FileScan csv [DR_NO#80,Date Rptd#81,DATE OCC#82,TIME OCC#83,AREA#84,AREA NAME#85,Rpt Dist No#86,Part 1-2#87,Crm Cd#88,Crm Cd Desc#89,Mocodes#90,Vict Age#91,Vict Sex#92,Vict Descent#93,Premis Cd#94,Premis Desc#95,Weapon Used Cd#96,Weapon Desc#97,Status#98,Status Desc#99,Crm Cd 1#100,Crm Cd 2#101,Crm Cd 3#102,Crm Cd 4#103,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=53]\n",
      "      +- Project [X#164, Y#165, FID#166, DIVISION#167, LOCATION#168, PREC#169,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_point#206]\n",
      "         +- FileScan csv [X#164,Y#165,FID#166,DIVISION#167,LOCATION#168,PREC#169] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:double,Y:double,FID:int,DIVISION:string,LOCATION:string,PREC:int>\n",
      "\n",
      "\n",
      "Overall time: 1.97 sec\n",
      "+----------------+--------------+-----------+\n",
      "|        DIVISION|avg_distance_m|crime_count|\n",
      "+----------------+--------------+-----------+\n",
      "|       HOLLYWOOD|       2073.58|     224124|\n",
      "|        VAN NUYS|       2939.26|     208129|\n",
      "|       SOUTHWEST|       2191.47|     189119|\n",
      "|        WILSHIRE|        2593.3|     186383|\n",
      "|     77TH STREET|       1717.11|     170620|\n",
      "| NORTH HOLLYWOOD|        2642.5|     168096|\n",
      "|         OLYMPIC|       1728.93|     162805|\n",
      "|         PACIFIC|        3853.5|     162027|\n",
      "|         CENTRAL|        993.32|     154689|\n",
      "|         RAMPART|       1534.22|     153204|\n",
      "|       SOUTHEAST|       2443.91|     143803|\n",
      "|     WEST VALLEY|       3021.57|     136622|\n",
      "|        FOOTHILL|        4260.1|     132482|\n",
      "|         TOPANGA|       3296.99|     131054|\n",
      "|          HARBOR|       3701.72|     127071|\n",
      "|      HOLLENBECK|     333909.08|     119381|\n",
      "|WEST LOS ANGELES|       2789.52|     115969|\n",
      "|          NEWTON|       1635.23|     111392|\n",
      "|       NORTHEAST|       3622.99|     108243|\n",
      "|         MISSION|       3676.23|      97926|\n",
      "+----------------+--------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import round, col, broadcast, row_number, avg, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, year, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, count, sum as Fsum\n",
    "import time\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session configuration (4 cores, 8 GB per executor)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime Data - Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Schemas\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "police_schema = StructType([\n",
    "    StructField(\"X\", DoubleType()),          \n",
    "    StructField(\"Y\", DoubleType()),          \n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load crime data and union\n",
    "crime_df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "crime_df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=True,\n",
    "    schema=crime_schema\n",
    ")\n",
    "\n",
    "crimes_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Load stations\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\", header=True, schema=police_schema)\n",
    "\n",
    "# Create POINT geometries\n",
    "crimes_df = crimes_df \\\n",
    "    .withColumn(\"crime_point\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "stations_df = stations_df \\\n",
    "    .withColumn(\"station_point\", ST_Point(col(\"X\"), col(\"Y\")))\n",
    "\n",
    "# Cross join crimes ↔ stations\n",
    "joined_df = crimes_df.crossJoin(stations_df)\n",
    "\n",
    "joined_df.explain(True)\n",
    "\n",
    "# Distance computation\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"distance_m\",\n",
    "    ST_DistanceSphere(col(\"crime_point\"), col(\"station_point\"))\n",
    ")\n",
    "\n",
    "# Nearest station per crime\n",
    "window = Window.partitionBy(\"DR_NO\").orderBy(asc(\"distance_m\"))\n",
    "\n",
    "closest_df = joined_df.withColumn(\n",
    "    \"rn\",\n",
    "    row_number().over(window)\n",
    ").filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# Aggregation per division\n",
    "result_df = closest_df.groupBy(\"DIVISION\").agg(\n",
    "    round(avg(\"distance_m\"),2).alias(\"avg_distance_m\"),\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ").orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nOverall time: {end - start:.2f} sec\")\n",
    "\n",
    "# Reorder and show\n",
    "result_df = result_df.select(\n",
    "    \"DIVISION\",\n",
    "    \"avg_distance_m\",\n",
    "    \"crime_count\"   # εδώ το βάζεις τελευταίο\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b1ed3-6bc6-4e6b-9e30-25491813a88c",
   "metadata": {},
   "source": [
    "## Query 5 – Income vs crime per capita across LA communities (COMM)\n",
    "\n",
    "Goal:\n",
    "\n",
    "Using LA census blocks, income data, and crime data:\n",
    "\n",
    "1. Compute per-capita income per community (COMM) based on:\n",
    "\n",
    " - Census block population (POP20)\n",
    "\n",
    " - Housing units (HOUSING20)\n",
    "\n",
    " - Median income per ZIP (MedianIncome)\n",
    "\n",
    "2. Spatially join crimes (2020–2021) with census blocks and:\n",
    "\n",
    " - Count crimes per COMM\n",
    "\n",
    " - Compute crime per capita per year per community\n",
    "\n",
    "3. Study the correlation between:\n",
    "\n",
    " - Per-capita income\n",
    "\n",
    " - Crime-per-capita-per-year\n",
    "\n",
    "4. Repeat under 3 cluster configurations, and separately analyze:\n",
    "\n",
    " - All communities\n",
    "\n",
    " - Top-10 richest communities\n",
    "\n",
    " - Bottom-10 poorest communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a214b6-8958-4f7d-aa65-b0d12e092025",
   "metadata": {},
   "source": [
    " ### 5.1 All communities – Config 1 (2 executors × 4 cores, 8 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2013b99-ebb8-4a50-921d-b09001520ea3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>164</td><td>application_1765289937462_0161</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0161/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0161_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9854bf0228a94f3b8afbfcba834367f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f82a4976a524d76bcbf746554d9d5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Join blocks_df ? income_df (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [ZCTA20])\n",
      ":- Filter isnotnull(block_geom#57)\n",
      ":  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      ":     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      ":        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      ":           +- Project [feat#33]\n",
      ":              +- Generate explode(features#25), false, [feat#33]\n",
      ":                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "+- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "   +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "      +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "         +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ZCTA20: string, COMM: string, POP20: bigint, HOUSING20: bigint, geometry_json: string, block_geom: geometry, MedianIncome: double\n",
      "Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "+- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "   :- Filter isnotnull(block_geom#57)\n",
      "   :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "   :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "   :           +- Project [feat#33]\n",
      "   :              +- Generate explode(features#25), false, [feat#33]\n",
      "   :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "   +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "      +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "         +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "            +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "+- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "   :- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   :  +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "   :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "   :        +- Project [features#25]\n",
      "   :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "   :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "   +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "      +- Filter isnotnull(Zip Code#82)\n",
      "         +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "   +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "      :- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      :  +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      :     +- Generate explode(features#25), false, [feat#33]\n",
      "      :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=94]\n",
      "         +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            +- Filter isnotnull(Zip Code#82)\n",
      "               +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "=== EXPLAIN: Spatial Join crime_df_filtered ? blocks_df (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=197]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Join comm_income_df ? comm_crime_df (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [COMM])\n",
      ":- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      ":  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      ":     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      ":        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      ":           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      ":              :- Filter isnotnull(block_geom#57)\n",
      ":              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      ":              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      ":              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      ":              :           +- Project [feat#33]\n",
      ":              :              +- Generate explode(features#25), false, [feat#33]\n",
      ":              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      ":              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      ":                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      ":                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      ":                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "+- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "   +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "      +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "         :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "         :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "         :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "         :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "         :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "         :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "         :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "         +- Filter isnotnull(block_geom#57)\n",
      "            +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                  +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                     +- Project [feat#33]\n",
      "                        +- Generate explode(features#290), false, [feat#33]\n",
      "                           +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "+- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "   :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "   :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "   :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "   :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "   :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "   :              :- Filter isnotnull(block_geom#57)\n",
      "   :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "   :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "   :              :           +- Project [feat#33]\n",
      "   :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "   :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "   :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "   :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "   :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "   :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "   +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "      +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "         +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "            :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "            :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "            :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "            :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "            :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "            :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "            :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "            +- Filter isnotnull(block_geom#57)\n",
      "               +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                  +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                        +- Project [feat#33]\n",
      "                           +- Generate explode(features#290), false, [feat#33]\n",
      "                              +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "+- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "   :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "   :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "   :     +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "   :        +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "   :           :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "   :           :  +- Generate explode(features#25), [0], false, [feat#33]\n",
      "   :           :     +- Project [features#25]\n",
      "   :           :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "   :           :           +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "   :           +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "   :              +- Filter isnotnull(Zip Code#82)\n",
      "   :                 +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "   +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "      +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "         +- Project [COMM#293]\n",
      "            +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "               :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "               :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "               :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "               +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                  +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                     +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                        +- Project [features#290]\n",
      "                           +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                              +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "   +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "      :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "      :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "      :     +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=358]\n",
      "      :           +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#306, sum#308L], schema specialized)\n",
      "      :              +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "      :                 +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "      :                    :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "      :                    :  +- Generate explode(features#25), false, [feat#33]\n",
      "      :                    :     +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                    :        +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "      :                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=353]\n",
      "      :                       +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "      :                          +- Filter isnotnull(Zip Code#82)\n",
      "      :                             +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "               +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=361]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#310L], schema specialized)\n",
      "                     +- Project [COMM#293]\n",
      "                        +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                           :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                           :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                           :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                           +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                              +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                                 +- Generate explode(features#290), false, [feat#33]\n",
      "                                    +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                       +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|COMM                                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|Franklin Canyon                       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula                |100709.34075104311|0.0                  |\n",
      "|Marina del Rey                        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades                     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula                      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands                   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista                           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air                               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island                 |74903.62352941177 |0.0                  |\n",
      "|Brentwood                             |72946.3541900178  |0.03533988263994198  |\n",
      "|Beverly Crest                         |69669.54254069475 |0.02584326229233093  |\n",
      "|Westfield/Academy Hills               |68003.89684813753 |0.0                  |\n",
      "|Mandeville Canyon                     |65870.0709438618  |0.015885256014805674 |\n",
      "|Venice                                |63440.7489833752  |0.09427700035880876  |\n",
      "|Carthay                               |62780.30752840909 |0.0747159090909091   |\n",
      "|Bouquet Canyon                        |60732.07149321267 |0.0                  |\n",
      "|San Francisquito Canyon/Bouquet Canyon|60634.46153846154 |0.0                  |\n",
      "|Playa Del Rey                         |60390.17038539554 |0.06389452332657201  |\n",
      "|Century City                          |58903.81757352941 |0.05058823529411765  |\n",
      "|Padua Hills                           |58891.90476190476 |0.0                  |\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Correlation (all COMM): -0.06799913590681889\n",
      "[Config 1] Total Query 5 runtime: 86.16 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time  \n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark & Sedona configuration (Config 1)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config1 (2x4, 8GB)\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Start timing AFTER Spark is ready\n",
    "t_query_start = time.time()  \n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean \"MedianIncome\" from strings like \"$75,123\" → 75123.0\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# ---- EXPLAIN for Join 1 (blocks ⨝ income) ----\n",
    "print(\"\\n=== EXPLAIN: Join blocks_df ⨝ income_df (Config 1) ===\")\n",
    "blocks_income_df.explain(True)\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: assign each crime to the block whose geometry contains it\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- EXPLAIN for spatial Join 2 (crimes ⨝ blocks) ----\n",
    "print(\"\\n=== EXPLAIN: Spatial Join crime_df_filtered ⨝ blocks_df (Config 1) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "# ---- EXPLAIN for Join 3 (comm_income ⨝ comm_crime) ----\n",
    "print(\"\\n=== EXPLAIN: Join comm_income_df ⨝ comm_crime_df (Config 1) ===\")\n",
    "comm_final_df.explain(True)\n",
    "\n",
    "# Replace NULL crime counts with 0 and compute crime per capita per year\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# Show top 20 COMM by income, with crime per capita\n",
    "comm_final_df.select(\"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\") \\\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc()) \\\n",
    "    .show(20, truncate=False)\n",
    "\n",
    "# ===== Correlation (all communities) =====\n",
    "\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "corr_all = comm_final_df_clean.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (all COMM):\", corr_all)\n",
    "\n",
    "# End timing AFTER last action\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 1] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3bc03-aa3c-40bf-b0ca-f867301e09e4",
   "metadata": {},
   "source": [
    "### 5.2 Top-10 richest communities – Config 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532db508-d689-4a02-b859-028948292cd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>336</td><td>application_1765289937462_0333</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0333/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0333_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ce9c6c87e6400f93354e5c3897ce0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047fc761d76d42488607df487ee45192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 1 - Top 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Top 10 Plan (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== TOP 10 richest COMMs ===\n",
      "+----------------------+------------------+---------------------+\n",
      "|COMM                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+----------------------+------------------+---------------------+\n",
      "|Franklin Canyon       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula|100709.34075104311|0.0                  |\n",
      "|Marina del Rey        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island |74903.62352941177 |0.0                  |\n",
      "|Brentwood             |72946.3541900178  |0.03533988263994198  |\n",
      "+----------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Top 10 richest): -0.5016218976503137\n",
      "[Config 1 - Top10] Total Query 5 runtime: 83.46 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark & Sedona configuration (Config 1 – Top 10)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config1 (2x4, 8GB) - Top10\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# EXPLAIN for SPATIAL JOIN\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 1 - Top 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === TOP 10 richest COMMs ===\n",
    "top10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "top10_list = [r[\"COMM\"] for r in top10.collect()]\n",
    "\n",
    "top10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(top10_list))\n",
    "\n",
    "\n",
    "# [ EXPLAIN for final TOP 10]\n",
    "print(\"\\n=== EXPLAIN: Final Top 10 Plan (Config 1) ===\")\n",
    "top10_df.explain(True)\n",
    "\n",
    "print(\"=== TOP 10 richest COMMs ===\")\n",
    "top10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").desc()).show(truncate=False)\n",
    "\n",
    "corr_top10 = top10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Top 10 richest):\", corr_top10)\n",
    "\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 1 - Top10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78816f94-a934-4e64-91e0-e467cacb5495",
   "metadata": {},
   "source": [
    "### 5.3 Bottom-10 poorest communities – Config 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e048e5-e60d-402a-83a3-9c7fd2925e49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>365</td><td>application_1765289937462_0362</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0362/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0362_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8945d7ed9b344e1d823bedc652b5700e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23df71715de45e383f71b18b663e1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 1 - Bottom 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Bottom 10 Plan (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== BOTTOM 10 poorest COMMs ===\n",
      "+-------------------+------------------+---------------------+\n",
      "|COMM               |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+-------------------+------------------+---------------------+\n",
      "|San Clemente Island|0.0               |0.0                  |\n",
      "|Lynwood            |0.0               |0.0                  |\n",
      "|Lakewood           |0.0               |0.0                  |\n",
      "|West LA            |4297.9052013422815|0.05620805369127517  |\n",
      "|Pomona             |6572.923898071625 |0.0                  |\n",
      "|Whittier Narrows   |8708.333333333334 |0.0                  |\n",
      "|Vernon Central     |10950.147886890725|0.04502019808477044  |\n",
      "|South Park         |11413.401021995112|0.05472395023328149  |\n",
      "|Central            |11541.450975876562|0.04928320571132477  |\n",
      "|Watts              |11751.343251432088|0.0565970742792217   |\n",
      "+-------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Bottom 10 poorest): 0.6944437780582208\n",
      "[Config 1 - Bottom10] Total Query 5 runtime: 92.45 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time \n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark & Sedona configuration (Config 1 – Bottom 10)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config1 (2x4, 8GB) - Bottom10\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# ---- Start timing (after Spark is ready) ----\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 1 - Bottom 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === BOTTOM 10 poorest COMMs ===\n",
    "bottom10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").asc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "bottom10_list = [r[\"COMM\"] for r in bottom10.collect()]\n",
    "\n",
    "bottom10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(bottom10_list))\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Final Bottom 10 Plan (Config 1) ===\")\n",
    "bottom10_df.explain(True)\n",
    "\n",
    "\n",
    "print(\"=== BOTTOM 10 poorest COMMs ===\")\n",
    "bottom10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").asc()).show(truncate=False)\n",
    "\n",
    "corr_bottom10 = bottom10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Bottom 10 poorest):\", corr_bottom10)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 1 - Bottom10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf47e3-950e-474c-b3a6-b1740f2949e7",
   "metadata": {},
   "source": [
    "### 5.4 All communities – Config 2 (4 executors × 2 cores, 4 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b4dcd5-9c49-410b-b5fd-c154bb8870b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>176</td><td>application_1765289937462_0173</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0173/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-103.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0173_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9d4cd3ac6e41b3a36a4d6823290ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67cea786e04d34a991b92a2a104753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN plan for spatial join crime_blocks_df (Config 2) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN plan for final join comm_final_df (Config 2) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, ('CrimePerYear / 'TotalPopulation) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Filter isnotnull(block_geom#57)\n",
      "         :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "         :              :           +- Project [feat#33]\n",
      "         :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "         :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "         :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "         :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                  :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                  :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                  :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                  :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                  :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Filter isnotnull(block_geom#57)\n",
      "                     +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                        +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                           +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                              +- Project [feat#33]\n",
      "                                 +- Generate explode(features#290), false, [feat#33]\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Filter isnotnull(block_geom#57)\n",
      "         :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "         :              :           +- Project [feat#33]\n",
      "         :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "         :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "         :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "         :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                  :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                  :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                  :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                  :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                  :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Filter isnotnull(block_geom#57)\n",
      "                     +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                        +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                           +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                              +- Project [feat#33]\n",
      "                                 +- Generate explode(features#290), false, [feat#33]\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "      :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "      :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "      :     +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "      :        +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "      :           :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "      :           :  +- Generate explode(features#25), [0], false, [feat#33]\n",
      "      :           :     +- Project [features#25]\n",
      "      :           :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :           :           +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "      :           +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "      :              +- Filter isnotnull(Zip Code#82)\n",
      "      :                 +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "      +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "         +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "            +- Project [COMM#293]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                  :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                     +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                        +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                           +- Project [features#290]\n",
      "                              +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                 +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "         :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "         :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :     +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "         :        +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "         :           +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#321, sum#323L], schema specialized)\n",
      "         :              +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :                 +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "         :                    :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :                    :  +- Generate explode(features#25), false, [feat#33]\n",
      "         :                    :     +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :                    :        +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "         :                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=271]\n",
      "         :                       +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                          +- Filter isnotnull(Zip Code#82)\n",
      "         :                             +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "         +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "            +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "               +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "                     +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#325L], schema specialized)\n",
      "                        +- Project [COMM#293]\n",
      "                           +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                              :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                              :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                              :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                              +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                 +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                                    +- Generate explode(features#290), false, [feat#33]\n",
      "                                       +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                          +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|COMM                                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|Franklin Canyon                       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula                |100709.34075104311|0.0                  |\n",
      "|Marina del Rey                        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades                     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula                      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands                   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista                           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air                               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island                 |74903.62352941177 |0.0                  |\n",
      "|Brentwood                             |72946.3541900178  |0.03533988263994198  |\n",
      "|Beverly Crest                         |69669.54254069475 |0.02584326229233093  |\n",
      "|Westfield/Academy Hills               |68003.89684813753 |0.0                  |\n",
      "|Mandeville Canyon                     |65870.0709438618  |0.015885256014805674 |\n",
      "|Venice                                |63440.7489833752  |0.09427700035880876  |\n",
      "|Carthay                               |62780.30752840909 |0.0747159090909091   |\n",
      "|Bouquet Canyon                        |60732.07149321267 |0.0                  |\n",
      "|San Francisquito Canyon/Bouquet Canyon|60634.46153846154 |0.0                  |\n",
      "|Playa Del Rey                         |60390.17038539554 |0.06389452332657201  |\n",
      "|Century City                          |58903.81757352941 |0.05058823529411765  |\n",
      "|Padua Hills                           |58891.90476190476 |0.0                  |\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Correlation (all COMM): -0.06799913590681889\n",
      "[Config 2 - All COMM] Total Query 5 runtime: 82.58 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time  \n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark & Sedona configuration (Config 2)\n",
    "# ---------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config2 (4x2, 4GB) - All COMM\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# ---- Start timing AFTER Spark is ready ----\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== EXPLAIN PLANS (for the report) ======================\n",
    "\n",
    "print(\"\\n=== EXPLAIN plan for spatial join crime_blocks_df (Config 2) ===\")\n",
    "crime_blocks_df.explain(mode=\"extended\")\n",
    "\n",
    "print(\"\\n=== EXPLAIN plan for final join comm_final_df (Config 2) ===\")\n",
    "comm_final_df.explain(mode=\"extended\")\n",
    "\n",
    "# ====================== RESULTS & CORRELATION ======================\n",
    "\n",
    "# Show top 20 COMM by income, with crime per capita\n",
    "comm_final_df.select(\"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\") \\\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc()) \\\n",
    "    .show(20, truncate=False)\n",
    "\n",
    "# ===== Correlation =====\n",
    "\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "corr_all = comm_final_df_clean.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (all COMM):\", corr_all)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 2 - All COMM] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd84a9-ffa5-4f46-a9ab-ead764e87da1",
   "metadata": {},
   "source": [
    "### 5.5 Top-10 richest communities – Config 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00114dca-3389-4f52-b67c-8f244ead0cd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>340</td><td>application_1765289937462_0337</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0337/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0337_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4da07adb05440ebb5943016d7f7529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8430eda72b4815bf240d4168d2f0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 2 - Top 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Top 10 Plan (Config 1) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== TOP 10 richest COMMs ===\n",
      "+----------------------+------------------+---------------------+\n",
      "|COMM                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+----------------------+------------------+---------------------+\n",
      "|Franklin Canyon       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula|100709.34075104311|0.0                  |\n",
      "|Marina del Rey        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island |74903.62352941177 |0.0                  |\n",
      "|Brentwood             |72946.3541900178  |0.03533988263994198  |\n",
      "+----------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Top 10 richest): -0.5016218976503137\n",
      "[Config 2 - Top10] Total Query 5 runtime: 92.23 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# ---------------- Spark & Sedona ----------------\n",
    "\n",
    "# Spark session with configuration 2\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config2 (4x2, 4GB) - Top10 Richest\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Start timing AFTER Spark is ready\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# EXPLAIN for SPATIAL JOIN\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 2 - Top 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === TOP 10 richest COMMs ===\n",
    "top10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "top10_list = [r[\"COMM\"] for r in top10.collect()]\n",
    "\n",
    "top10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(top10_list))\n",
    "\n",
    "# EXPLAIN for final TOP 10\n",
    "print(\"\\n=== EXPLAIN: Final Top 10 Plan (Config 1) ===\")\n",
    "top10_df.explain(True)\n",
    "\n",
    "print(\"=== TOP 10 richest COMMs ===\")\n",
    "top10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").desc()).show(truncate=False)\n",
    "\n",
    "corr_top10 = top10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Top 10 richest):\", corr_top10)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 2 - Top10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1727e3-f48f-4292-8b55-a8cf7767c06e",
   "metadata": {},
   "source": [
    "### 5.6 Bottom-10 poorest communities – Config 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa23201-8afb-4949-835f-c0366efc9644",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>371</td><td>application_1765289937462_0368</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0368/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0368_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd18566bfd74f5a8ae674fd71eedc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63978a83ebf41509e7f536ab0a0be80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 2 - Bottom 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Bottom 10 Plan (Config 2) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== BOTTOM 10 poorest COMMs ===\n",
      "+-------------------+------------------+---------------------+\n",
      "|COMM               |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+-------------------+------------------+---------------------+\n",
      "|San Clemente Island|0.0               |0.0                  |\n",
      "|Lynwood            |0.0               |0.0                  |\n",
      "|Lakewood           |0.0               |0.0                  |\n",
      "|West LA            |4297.9052013422815|0.05620805369127517  |\n",
      "|Pomona             |6572.923898071625 |0.0                  |\n",
      "|Whittier Narrows   |8708.333333333334 |0.0                  |\n",
      "|Vernon Central     |10950.147886890725|0.04502019808477044  |\n",
      "|South Park         |11413.401021995112|0.05472395023328149  |\n",
      "|Central            |11541.450975876562|0.04928320571132477  |\n",
      "|Watts              |11751.343251432088|0.0565970742792217   |\n",
      "+-------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Bottom 10 poorest): 0.6944437780582208\n",
      "[Config 2 - Bottom10] Total Query 5 runtime: 83.29 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time   \n",
    "\n",
    "# ---------------- Spark & Sedona ----------------\n",
    "\n",
    "# Spark session with configuration 2\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config2 (4x2, 4GB) - Bottom10 Poorest\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# ---- Start timing (after Spark is ready) ----\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 2 - Bottom 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === BOTTOM 10 poorest COMMs ===\n",
    "bottom10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").asc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "bottom10_list = [r[\"COMM\"] for r in bottom10.collect()]\n",
    "\n",
    "bottom10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(bottom10_list))\n",
    "\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Final Bottom 10 Plan (Config 2) ===\")\n",
    "bottom10_df.explain(True)\n",
    "\n",
    "print(\"=== BOTTOM 10 poorest COMMs ===\")\n",
    "bottom10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").asc()).show(truncate=False)\n",
    "\n",
    "corr_bottom10 = bottom10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Bottom 10 poorest):\", corr_bottom10)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 2 - Bottom10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b0ecf-0bd1-4aa0-813a-ef4e4e199b46",
   "metadata": {},
   "source": [
    "### 5.7 All communities – Config 3 (8 executors × 1 cores, 2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf92c5ca-3bfc-4d2b-86e3-0889abb0a634",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>292</td><td>application_1765289937462_0289</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0289/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0289_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187260f0f1ed4e7fa7b7c400af7587ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab39df201ebf4eaf8c04a98a0a40c60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN plan for spatial join crime_blocks_df (Config 2) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN plan for final join comm_final_df (Config 2) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, ('CrimePerYear / 'TotalPopulation) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Filter isnotnull(block_geom#57)\n",
      "         :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "         :              :           +- Project [feat#33]\n",
      "         :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "         :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "         :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "         :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                  :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                  :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                  :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                  :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                  :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Filter isnotnull(block_geom#57)\n",
      "                     +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                        +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                           +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                              +- Project [feat#33]\n",
      "                                 +- Generate explode(features#290), false, [feat#33]\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Filter isnotnull(block_geom#57)\n",
      "         :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "         :              :           +- Project [feat#33]\n",
      "         :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "         :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "         :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "         :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                  :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                  :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                  :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                  :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                  :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Filter isnotnull(block_geom#57)\n",
      "                     +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                        +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                           +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                              +- Project [feat#33]\n",
      "                                 +- Generate explode(features#290), false, [feat#33]\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "      :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "      :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "      :     +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "      :        +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "      :           :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "      :           :  +- Generate explode(features#25), [0], false, [feat#33]\n",
      "      :           :     +- Project [features#25]\n",
      "      :           :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :           :           +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "      :           +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "      :              +- Filter isnotnull(Zip Code#82)\n",
      "      :                 +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "      +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "         +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "            +- Project [COMM#293]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                  :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                  :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                  +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                     +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                        +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                           +- Project [features#290]\n",
      "                              +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                 +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "         :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "         :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :     +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "         :        +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "         :           +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#321, sum#323L], schema specialized)\n",
      "         :              +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :                 +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "         :                    :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :                    :  +- Generate explode(features#25), false, [feat#33]\n",
      "         :                    :     +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :                    :        +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "         :                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=271]\n",
      "         :                       +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                          +- Filter isnotnull(Zip Code#82)\n",
      "         :                             +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "         +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "            +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "               +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "                     +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#325L], schema specialized)\n",
      "                        +- Project [COMM#293]\n",
      "                           +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                              :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                              :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                              :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                              +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                 +- Filter isnotnull(feat#33.properties.COMM)\n",
      "                                    +- Generate explode(features#290), false, [feat#33]\n",
      "                                       +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                          +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|COMM                                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "|Franklin Canyon                       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula                |100709.34075104311|0.0                  |\n",
      "|Marina del Rey                        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades                     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula                      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands                   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista                           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air                               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island                 |74903.62352941177 |0.0                  |\n",
      "|Brentwood                             |72946.3541900178  |0.03533988263994198  |\n",
      "|Beverly Crest                         |69669.54254069475 |0.02584326229233093  |\n",
      "|Westfield/Academy Hills               |68003.89684813753 |0.0                  |\n",
      "|Mandeville Canyon                     |65870.0709438618  |0.015885256014805674 |\n",
      "|Venice                                |63440.7489833752  |0.09427700035880876  |\n",
      "|Carthay                               |62780.30752840909 |0.0747159090909091   |\n",
      "|Bouquet Canyon                        |60732.07149321267 |0.0                  |\n",
      "|San Francisquito Canyon/Bouquet Canyon|60634.46153846154 |0.0                  |\n",
      "|Playa Del Rey                         |60390.17038539554 |0.06389452332657201  |\n",
      "|Century City                          |58903.81757352941 |0.05058823529411765  |\n",
      "|Padua Hills                           |58891.90476190476 |0.0                  |\n",
      "+--------------------------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Correlation (all COMM): -0.06799913590681889\n",
      "[Config 3 - All COMM] Total Query 5 runtime: 70.97 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# ---------------- Spark & Sedona ----------------\n",
    "\n",
    "# Spark session with configuration 3\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config3 (8x1, 2GB) - ALL COMM\")\n",
    "    .config(\"spark.executor.instances\", \"8\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# ---- Start timing AFTER Spark is ready ----\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== EXPLAIN PLANS (for the report) ======================\n",
    "\n",
    "print(\"\\n=== EXPLAIN plan for spatial join crime_blocks_df (Config 3) ===\")\n",
    "crime_blocks_df.explain(mode=\"extended\")\n",
    "\n",
    "print(\"\\n=== EXPLAIN plan for final join comm_final_df (Config 3) ===\")\n",
    "comm_final_df.explain(mode=\"extended\")\n",
    "\n",
    "# ====================== RESULTS & CORRELATION ======================\n",
    "\n",
    "# Show top 20 COMM by income, with crime per capita\n",
    "comm_final_df.select(\"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\") \\\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc()) \\\n",
    "    .show(20, truncate=False)\n",
    "\n",
    "# ===== Correlation =====\n",
    "\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "corr_all = comm_final_df_clean.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (all COMM):\", corr_all)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 3 - All COMM] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66944843-5188-4f44-9a72-eec34ec0d10a",
   "metadata": {},
   "source": [
    "### 5.8 Top-10 richest communities – Config 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26a7704-2c29-407c-b119-2641cf181742",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>344</td><td>application_1765289937462_0341</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0341/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0341_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd5d79f24c34c748019ca532d4e684b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a1479b548843d6a1a15072a9e09366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 3 - Top 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Top 10 Plan (Config 3) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (Franklin Canyon,Palos Verdes Peninsula,Marina del Rey,Pacific Palisades,Marina Peninsula,Palisades Highlands,Playa Vista,Bel Air,Santa Catalina Island,Brentwood) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== TOP 10 richest COMMs ===\n",
      "+----------------------+------------------+---------------------+\n",
      "|COMM                  |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+----------------------+------------------+---------------------+\n",
      "|Franklin Canyon       |154740.0          |0.0                  |\n",
      "|Palos Verdes Peninsula|100709.34075104311|0.0                  |\n",
      "|Marina del Rey        |97715.99683460829 |0.00294557284797327  |\n",
      "|Pacific Palisades     |87750.52529591447 |0.0325028636884307   |\n",
      "|Marina Peninsula      |85588.53457067102 |0.04843973077707526  |\n",
      "|Palisades Highlands   |83576.88954231654 |0.016236256711838405 |\n",
      "|Playa Vista           |81679.88040665434 |0.03986444855206408  |\n",
      "|Bel Air               |80412.74199793495 |0.034073309241094474 |\n",
      "|Santa Catalina Island |74903.62352941177 |0.0                  |\n",
      "|Brentwood             |72946.3541900178  |0.03533988263994198  |\n",
      "+----------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Top 10 richest): -0.5016218976503137\n",
      "[Config 3 - Top10] Total Query 5 runtime: 94.98 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time  \n",
    "\n",
    "# ---------------- Spark & Sedona ----------------\n",
    "\n",
    "# Spark session with configuration 3\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config3 (8x1, 2GB) - Top10 Richest\")\n",
    "    .config(\"spark.executor.instances\", \"8\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Start timing AFTER Spark is ready\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# EXPLAIN for SPATIAL JOIN\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 3 - Top 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === TOP 10 richest COMMs ===\n",
    "top10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").desc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "top10_list = [r[\"COMM\"] for r in top10.collect()]\n",
    "\n",
    "\n",
    "top10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(top10_list))\n",
    "\n",
    "\n",
    "# EXPLAIN for final TOP 10\n",
    "print(\"\\n=== EXPLAIN: Final Top 10 Plan (Config 3) ===\")\n",
    "top10_df.explain(True)\n",
    "\n",
    "print(\"=== TOP 10 richest COMMs ===\")\n",
    "top10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").desc()).show(truncate=False)\n",
    "\n",
    "corr_top10 = top10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Top 10 richest):\", corr_top10)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 3 - Top10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad71bc9-0f83-415c-81c9-c14ba7c34033",
   "metadata": {},
   "source": [
    "### 5.9 Bottom-10 poorest communities – Config 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68d59e8-c0a9-4b7c-b3f4-83922b5d8675",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>388</td><td>application_1765289937462_0384</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0384/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0384_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77edc2de09884792966a49bd09037d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6215d11a98ca47649f59d2a0be938a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLAIN: Spatial Join Plan (Config 3 - Bottom 10) ===\n",
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, LAT: double, LON: double, Date Rptd: string, DATE OCC: string, OCC_TS: timestamp, YEAR_OCC: int, crime_point: geometry, COMM: string, POP20: bigint, HOUSING20: bigint, ZCTA20: string, geometry_json: string, block_geom: geometry\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      ":     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      ":        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      ":           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      ":                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Filter isnotnull(block_geom#57)\n",
      "   +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "            +- Project [feat#33]\n",
      "               +- Generate explode(features#25), false, [feat#33]\n",
      "                  +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      ":- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      ":  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      ":     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      ":        +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "+- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "   +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "      +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         +- Project [features#25]\n",
      "            +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "               +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "   :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "   :  +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "   :     +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "   :        +- FileScan csv [DR_NO#153,Date Rptd#154,DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,LAT:double,LON:double>\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=109]\n",
      "      +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "         +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "            +- Generate explode(features#25), false, [feat#33]\n",
      "               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                  +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "\n",
      "=== EXPLAIN: Final Bottom 10 Plan (Config 3) ===\n",
      "== Parsed Logical Plan ==\n",
      "'Filter 'COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, TotalIncome: double, TotalPopulation: bigint, PerCapitaIncome: double, CrimeCount_2yrs: bigint, CrimePerYear: double, CrimePerCapitaPerYear: double\n",
      "Filter COMM#36 IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "+- Filter (isnotnull(PerCapitaIncome#130) AND isnotnull(CrimePerCapitaPerYear#312))\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "      +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "         +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#285]\n",
      "            +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "               :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "               :  +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "               :     +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "               :        +- Project [ZCTA20#39, COMM#36, POP20#37L, HOUSING20#38L, geometry_json#51, block_geom#57, MedianIncome#96]\n",
      "               :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "               :              :- Filter isnotnull(block_geom#57)\n",
      "               :              :  +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "               :              :     +- Project [COMM#36, POP20#37L, HOUSING20#38L, ZCTA20#39, regexp_replace(regexp_replace(geometry_json#40, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "               :              :        +- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#40]\n",
      "               :              :           +- Project [feat#33]\n",
      "               :              :              +- Generate explode(features#25), false, [feat#33]\n",
      "               :              :                 +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "               :              +- Project [ZCTA20#88, cast(regexp_replace(MedianIncome#93, ,, , 1) as double) AS MedianIncome#96]\n",
      "               :                 +- Project [ZCTA20#88, regexp_replace(MedianIncome#89, \\$, , 1) AS MedianIncome#93]\n",
      "               :                    +- Project [Zip Code#82 AS ZCTA20#88, Estimated Median Income#84 AS MedianIncome#89]\n",
      "               :                       +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / cast(2.0 as double)) AS CrimePerYear#285]\n",
      "                  +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "                     +- Join LeftOuter,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                        :- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, YEAR_OCC#222,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                        :  +- Filter NOT ((LAT#179 = cast(0 as double)) AND (LON#180 = cast(0 as double)))\n",
      "                        :     +- Filter YEAR_OCC#222 IN (2020,2021)\n",
      "                        :        +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, OCC_TS#215, year(cast(OCC_TS#215 as date)) AS YEAR_OCC#222]\n",
      "                        :           +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155, to_timestamp(DATE OCC#155, Some(yyyy MMM dd hh:mm:ss a), TimestampType, Some(UTC), false) AS OCC_TS#215]\n",
      "                        :              +- Project [DR_NO#153, LAT#179, LON#180, Date Rptd#154, DATE OCC#155]\n",
      "                        :                 +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                        +- Filter isnotnull(block_geom#57)\n",
      "                           +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, geometry_json#51, st_geomfromgeojson(geometry_json#51) AS block_geom#57]\n",
      "                              +- Project [COMM#293, POP20#294L, HOUSING20#295L, ZCTA20#296, regexp_replace(regexp_replace(geometry_json#297, \"\\[, [, 1), \\]\", ], 1) AS geometry_json#51]\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, feat#33.properties.POP20 AS POP20#294L, feat#33.properties.HOUSING20 AS HOUSING20#295L, feat#33.properties.ZCTA20 AS ZCTA20#296, to_json(feat#33.geometry, Some(UTC)) AS geometry_json#297]\n",
      "                                    +- Project [feat#33]\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "   +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "      +- Join LeftOuter, (COMM#36 = COMM#293)\n",
      "         :- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "         :  +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "         :     +- Aggregate [COMM#36], [COMM#36, sum(BlockIncome#106) AS TotalIncome#124, sum(POP20#37L) AS TotalPopulation#126L]\n",
      "         :        +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "         :           +- Join LeftOuter, (ZCTA20#39 = ZCTA20#88)\n",
      "         :              :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "         :              :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "         :              :     +- Generate explode(features#25), [0], false, [feat#33]\n",
      "         :              :        +- Project [features#25]\n",
      "         :              :           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "         :              :              +- Relation [crs#24,features#25,name#26,type#27] json\n",
      "         :              +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "         :                 +- Filter isnotnull(Zip Code#82)\n",
      "         :                    +- Relation [Zip Code#82,Community#83,Estimated Median Income#84] csv\n",
      "         +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "            +- Aggregate [COMM#293], [COMM#293, count(1) AS CrimeCount_2yrs#282L]\n",
      "               +- Project [COMM#293]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                     :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                     :     +- Relation [DR_NO#153,Date Rptd#154,DATE OCC#155,TIME OCC#156,AREA#157,AREA NAME#158,Rpt Dist No#159,Part 1-2#160,Crm Cd#161,Crm Cd Desc#162,Mocodes#163,Vict Age#164,Vict Sex#165,Vict Descent#166,Premis Cd#167,Premis Desc#168,Weapon Used Cd#169,Weapon Desc#170,Status#171,Status Desc#172,Crm Cd 1#173,Crm Cd 2#174,Crm Cd 3#175,Crm Cd 4#176,... 4 more fields] csv\n",
      "                     +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                        +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                           +- Generate explode(features#290), [0], false, [feat#33]\n",
      "                              +- Project [features#290]\n",
      "                                 +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                    +- Relation [crs#289,features#290,name#291,type#292] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, CrimePerYear#305, (CrimePerYear#305 / cast(TotalPopulation#126L as double)) AS CrimePerCapitaPerYear#312]\n",
      "   +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, PerCapitaIncome#130, CrimeCount_2yrs#282L, coalesce(CrimePerYear#285, 0.0) AS CrimePerYear#305]\n",
      "      +- Filter isnotnull((coalesce(CrimePerYear#285, 0.0) / cast(TotalPopulation#126L as double)))\n",
      "         +- SortMergeJoin [COMM#36], [COMM#293], LeftOuter\n",
      "            :- Sort [COMM#36 ASC NULLS FIRST], false, 0\n",
      "            :  +- Project [COMM#36, TotalIncome#124, TotalPopulation#126L, (TotalIncome#124 / cast(TotalPopulation#126L as double)) AS PerCapitaIncome#130]\n",
      "            :     +- Filter ((isnotnull(TotalIncome#124) AND isnotnull(TotalPopulation#126L)) AND isnotnull((TotalIncome#124 / cast(TotalPopulation#126L as double))))\n",
      "            :        +- HashAggregate(keys=[COMM#36], functions=[sum(BlockIncome#106), sum(POP20#37L)], output=[COMM#36, TotalIncome#124, TotalPopulation#126L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#36, 1000), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "            :              +- HashAggregate(keys=[COMM#36], functions=[partial_sum(BlockIncome#106), partial_sum(POP20#37L)], output=[COMM#36, sum#322, sum#324L], schema specialized)\n",
      "            :                 +- Project [COMM#36, POP20#37L, (cast(HOUSING20#38L as double) * MedianIncome#96) AS BlockIncome#106]\n",
      "            :                    +- BroadcastHashJoin [ZCTA20#39], [ZCTA20#88], LeftOuter, BuildRight, false\n",
      "            :                       :- Project [feat#33.properties.COMM AS COMM#36, feat#33.properties.POP20 AS POP20#37L, feat#33.properties.HOUSING20 AS HOUSING20#38L, feat#33.properties.ZCTA20 AS ZCTA20#39]\n",
      "            :                       :  +- Filter feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts)\n",
      "            :                       :     +- Generate explode(features#25), false, [feat#33]\n",
      "            :                       :        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                       :           +- FileScan json [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "            :                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=881]\n",
      "            :                          +- Project [Zip Code#82 AS ZCTA20#88, cast(regexp_replace(regexp_replace(Estimated Median Income#84, \\$, , 1), ,, , 1) as double) AS MedianIncome#96]\n",
      "            :                             +- Filter isnotnull(Zip Code#82)\n",
      "            :                                +- FileScan csv [Zip Code#82,Estimated Median Income#84] Batched: false, DataFilters: [isnotnull(Zip Code#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            +- Sort [COMM#293 ASC NULLS FIRST], false, 0\n",
      "               +- Project [COMM#293, CrimeCount_2yrs#282L, (cast(CrimeCount_2yrs#282L as double) / 2.0) AS CrimePerYear#285]\n",
      "                  +- HashAggregate(keys=[COMM#293], functions=[count(1)], output=[COMM#293, CrimeCount_2yrs#282L], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#293, 1000), ENSURE_REQUIREMENTS, [plan_id=890]\n",
      "                        +- HashAggregate(keys=[COMM#293], functions=[partial_count(1)], output=[COMM#293, count#326L], schema specialized)\n",
      "                           +- Project [COMM#293]\n",
      "                              +- RangeJoin crime_point#230: geometry, block_geom#57: geometry, WITHIN\n",
      "                                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#230]\n",
      "                                 :  +- Filter (year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as date)) IN (2020,2021) AND (NOT (LAT#179 = 0.0) OR NOT (LON#180 = 0.0)))\n",
      "                                 :     +- FileScan csv [DATE OCC#155,LAT#179,LON#180] Batched: false, DataFilters: [year(cast(gettimestamp(DATE OCC#155, yyyy MMM dd hh:mm:ss a, TimestampType, Some(UTC), false) as..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0)))], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "                                 +- Project [feat#33.properties.COMM AS COMM#293, st_geomfromgeojson(regexp_replace(regexp_replace(to_json(feat#33.geometry, Some(UTC)), \"\\[, [, 1), \\]\", ], 1)) AS block_geom#57]\n",
      "                                    +- Filter (feat#33.properties.COMM IN (San Clemente Island,Lynwood,Lakewood,West LA,Pomona,Whittier Narrows,Vernon Central,South Park,Central,Watts) AND isnotnull(feat#33.properties.COMM))\n",
      "                                       +- Generate explode(features#290), false, [feat#33]\n",
      "                                          +- Filter ((size(features#290, true) > 0) AND isnotnull(features#290))\n",
      "                                             +- FileScan json [features#290] Batched: false, DataFilters: [(size(features#290, true) > 0), isnotnull(features#290)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "\n",
      "=== BOTTOM 10 poorest COMMs ===\n",
      "+-------------------+------------------+---------------------+\n",
      "|COMM               |PerCapitaIncome   |CrimePerCapitaPerYear|\n",
      "+-------------------+------------------+---------------------+\n",
      "|San Clemente Island|0.0               |0.0                  |\n",
      "|Lynwood            |0.0               |0.0                  |\n",
      "|Lakewood           |0.0               |0.0                  |\n",
      "|West LA            |4297.9052013422815|0.05620805369127517  |\n",
      "|Pomona             |6572.923898071625 |0.0                  |\n",
      "|Whittier Narrows   |8708.333333333334 |0.0                  |\n",
      "|Vernon Central     |10950.147886890725|0.04502019808477044  |\n",
      "|South Park         |11413.401021995112|0.05472395023328149  |\n",
      "|Central            |11541.450975876562|0.04928320571132477  |\n",
      "|Watts              |11751.343251432088|0.0565970742792217   |\n",
      "+-------------------+------------------+---------------------+\n",
      "\n",
      "Correlation (Bottom 10 poorest): 0.6944437780582208\n",
      "[Config 3 - Bottom10] Total Query 5 runtime: 81.13 seconds"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sedona.sql import ST_Point, ST_GeomFromGeoJSON, ST_Contains\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time  \n",
    "\n",
    "# ---------------- Spark & Sedona ----------------\n",
    "\n",
    "# Spark session with configuration 3\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Query5 - Config3 (8x1, 2GB) - Bottom10 Poorest\")\n",
    "    .config(\"spark.executor.instances\", \"8\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# ---- Start timing (after Spark is ready) ----\n",
    "t_query_start = time.time()\n",
    "\n",
    "# ====================== INCOME / BLOCKS ======================\n",
    "\n",
    "# 1. Read the GeoJSON FeatureCollection as JSON\n",
    "raw_blocks_df = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\")\n",
    ")\n",
    "\n",
    "# 2. Explode \"features\" → one row per feature\n",
    "features_df = raw_blocks_df.select(F.explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "# 3. Select needed properties + geometry, then convert geometry struct → GeoJSON string\n",
    "#    and FIX malformed coordinate strings like \"[-118.53,32.90]\" → [-118.53,32.90]\n",
    "blocks_df = (\n",
    "    features_df\n",
    "    .select(\n",
    "        F.col(\"feat.properties.COMM\").alias(\"COMM\"),\n",
    "        F.col(\"feat.properties.POP20\").alias(\"POP20\"),\n",
    "        F.col(\"feat.properties.HOUSING20\").alias(\"HOUSING20\"),\n",
    "        F.col(\"feat.properties.ZCTA20\").alias(\"ZCTA20\"),\n",
    "        F.to_json(F.col(\"feat.geometry\")).alias(\"geometry_json\")\n",
    "    )\n",
    "    # fix the \"coordinates\" string issue WITHOUT UDFs\n",
    "    .withColumn(\n",
    "        \"geometry_json\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.col(\"geometry_json\"),\n",
    "                '\"\\\\[',   # turn \"[-118,... into [-118,...\n",
    "                '['\n",
    "            ),\n",
    "            '\\\\]\"',      # turn ...]\" into ...]\n",
    "            ']'\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"block_geom\",\n",
    "        ST_GeomFromGeoJSON(F.col(\"geometry_json\"))\n",
    "    )\n",
    "    .filter(F.col(\"block_geom\").isNotNull())\n",
    ")\n",
    "\n",
    "# 4. Income CSV - median income per ZIP code\n",
    "income_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\",\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"ZCTA20\"),\n",
    "        F.col(\"Estimated Median Income\").alias(\"MedianIncome\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Clean MedianIncome to numeric\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \"\\\\$\", \"\")\n",
    ")\n",
    "income_df = income_df.withColumn(\n",
    "    \"MedianIncome\",\n",
    "    F.regexp_replace(\"MedianIncome\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5. Join blocks with income by ZCTA20\n",
    "blocks_income_df = blocks_df.join(income_df, on=\"ZCTA20\", how=\"left\")\n",
    "\n",
    "# 6. Block income = HOUSING20 * MedianIncome\n",
    "blocks_income_df = blocks_income_df.withColumn(\n",
    "    \"BlockIncome\",\n",
    "    F.col(\"HOUSING20\") * F.col(\"MedianIncome\")\n",
    ")\n",
    "\n",
    "# 7. Aggregate per COMM\n",
    "comm_income_df = blocks_income_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"BlockIncome\").alias(\"TotalIncome\"),\n",
    "    F.sum(\"POP20\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# 8. Per capita income per COMM\n",
    "comm_income_df = comm_income_df.withColumn(\n",
    "    \"PerCapitaIncome\",\n",
    "    F.col(\"TotalIncome\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ====================== CRIMES ======================\n",
    "\n",
    "crime_df = (\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .select(\n",
    "        \"DR_NO\", \"LAT\", \"LON\", \"Date Rptd\", \"DATE OCC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parse DATE OCC as timestamp\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"OCC_TS\",\n",
    "    F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Year of occurrence\n",
    "crime_df = crime_df.withColumn(\"YEAR_OCC\", F.year(F.col(\"OCC_TS\")))\n",
    "\n",
    "# Keep only crimes in 2020 & 2021\n",
    "crime_df_filtered = crime_df.filter(F.col(\"YEAR_OCC\").isin(2020, 2021))\n",
    "\n",
    "# Filter out (0,0) coordinates\n",
    "crime_df_filtered = crime_df_filtered.filter(\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Create crime point geometry (lon, lat)\n",
    "crime_df_filtered = crime_df_filtered.withColumn(\n",
    "    \"crime_point\",\n",
    "    ST_Point(F.col(\"LON\"), F.col(\"LAT\"))\n",
    ")\n",
    "\n",
    "# ====================== SPATIAL JOIN & METRICS ======================\n",
    "\n",
    "# Spatial join: which block each crime falls into\n",
    "crime_blocks_df = crime_df_filtered.join(\n",
    "    blocks_df,\n",
    "    ST_Contains(F.col(\"block_geom\"), F.col(\"crime_point\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Spatial Join Plan (Config 3 - Bottom 10) ===\")\n",
    "crime_blocks_df.explain(True)\n",
    "\n",
    "# Crimes per COMM over 2 years\n",
    "comm_crime_df = crime_blocks_df.groupBy(\"COMM\").agg(\n",
    "    F.count(\"*\").alias(\"CrimeCount_2yrs\")\n",
    ")\n",
    "\n",
    "# Annual crimes per COMM (average over 2020–2021)\n",
    "comm_crime_df = comm_crime_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.col(\"CrimeCount_2yrs\") / F.lit(2.0)\n",
    ")\n",
    "\n",
    "# Final table: income + crime metrics\n",
    "comm_final_df = comm_income_df.join(comm_crime_df, on=\"COMM\", how=\"left\")\n",
    "\n",
    "comm_final_df = comm_final_df.withColumn(\n",
    "    \"CrimePerYear\",\n",
    "    F.coalesce(F.col(\"CrimePerYear\"), F.lit(0.0))\n",
    ").withColumn(\n",
    "    \"CrimePerCapitaPerYear\",\n",
    "    F.col(\"CrimePerYear\") / F.col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# ===== Correlation base DF (non-null) =====\n",
    "comm_final_df_clean = comm_final_df.filter(\n",
    "    F.col(\"PerCapitaIncome\").isNotNull() &\n",
    "    F.col(\"CrimePerCapitaPerYear\").isNotNull()\n",
    ")\n",
    "\n",
    "# === BOTTOM 10 poorest COMMs ===\n",
    "bottom10 = (\n",
    "    comm_final_df_clean\n",
    "    .orderBy(F.col(\"PerCapitaIncome\").asc())\n",
    "    .limit(10)\n",
    "    .select(\"COMM\")\n",
    ")\n",
    "\n",
    "bottom10_list = [r[\"COMM\"] for r in bottom10.collect()]\n",
    "\n",
    "bottom10_df = comm_final_df_clean.filter(F.col(\"COMM\").isin(bottom10_list))\n",
    "\n",
    "print(\"\\n=== EXPLAIN: Final Bottom 10 Plan (Config 3) ===\")\n",
    "bottom10_df.explain(True)\n",
    "\n",
    "print(\"=== BOTTOM 10 poorest COMMs ===\")\n",
    "bottom10_df.select(\n",
    "    \"COMM\", \"PerCapitaIncome\", \"CrimePerCapitaPerYear\"\n",
    ").orderBy(F.col(\"PerCapitaIncome\").asc()).show(truncate=False)\n",
    "\n",
    "corr_bottom10 = bottom10_df.stat.corr(\"PerCapitaIncome\", \"CrimePerCapitaPerYear\")\n",
    "print(\"Correlation (Bottom 10 poorest):\", corr_bottom10)\n",
    "\n",
    "# ---- End timing ----\n",
    "t_query_end = time.time()\n",
    "print(f\"[Config 3 - Bottom10] Total Query 5 runtime: {t_query_end - t_query_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6939e-935a-46a8-807f-3e6840d397af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
